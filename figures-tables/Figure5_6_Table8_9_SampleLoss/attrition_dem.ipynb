{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "sns.axes_style(\"whitegrid\")\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "from datetime import datetime\n",
    "import nbimporter\n",
    "from scipy import stats\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('..','..','data')\n",
    "# core sample\n",
    "core_data =  pd.read_csv(os.path.join(data_dir, 'core_dem.csv'), dtype = str)\n",
    "core_data['prlfc_dem_age'] = pd.to_numeric(core_data['prlfc_dem_age'])\n",
    "# excluded sample\n",
    "excluded_data =  pd.read_csv(os.path.join(data_dir, 'excluded_dem.csv'), dtype = str)\n",
    "excluded_data['prlfc_dem_age'] = pd.to_numeric(excluded_data['prlfc_dem_age'])\n",
    "# low completion sample\n",
    "lowCompl_data =  pd.read_csv(os.path.join(data_dir, 'lowCompl_dem.csv'), dtype = str)\n",
    "lowCompl_data['prlfc_dem_age'] = pd.to_numeric(lowCompl_data['prlfc_dem_age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS extract demographics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_age_sex_data(list_of_ids, survey_data,column_extention):\n",
    "    \"\"\"\n",
    "    Gathers age and sex data from survey and census based on specific IDs\n",
    "    \n",
    "    Args:\n",
    "        list_of_ids (list or array): list of ids to be included\n",
    "        survey_data (pd.DataFrame): dataframe containing survey data for one week\n",
    "        census_data (pd.DataFrame): dataframe containing census information\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): survey data\n",
    "        df2 ((pd.DataFrame): census data\n",
    "        ages (list): list of age ranges as strings\n",
    "    \"\"\"\n",
    "    \n",
    "    ids = [value for value in list_of_ids if value in list(survey_data['PROLIFIC_PID'])]\n",
    "    survey_data = survey_data.set_index('PROLIFIC_PID')\n",
    "    \n",
    "    survey = survey_data[['prlfc_dem_age', 'DemC5']].loc[ids].astype(float)\n",
    "    survey.columns = ['Age', 'Sex']\n",
    "    bins= [i*5 for i in range(21)]\n",
    "    labels = [str(i*5) + '-' + str((i+1)*5-1) for i in range(20)]\n",
    "\n",
    "    survey['Age'] = pd.cut(survey['Age'], bins=bins, labels=labels, right=False)\n",
    "    survey = survey.set_index('Age')\n",
    "\n",
    "    ages = [value for value in labels if value in list(survey.index)]\n",
    "\n",
    "    df_dict = {'Age':ages, 'Male':[], 'Female':[]}\n",
    "\n",
    "    for age in ages:\n",
    "        to_count = survey['Sex'].loc[age]\n",
    "        if type(to_count) == np.float64:\n",
    "                to_count = [to_count]\n",
    "        counter = Counter(to_count)\n",
    "        df_dict['Male'].append(counter[1.0])\n",
    "        df_dict['Female'].append(-counter[2.0])\n",
    "\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    df = df.set_index('Age')\n",
    "    total = sum(df['Male']) - sum(df['Female'])\n",
    "    df['Male'] = df['Male']/total * 100\n",
    "    df['Female'] = df['Female']/total * 100\n",
    "    \n",
    "    df['Male'] = df['Male'].round(decimals=2)\n",
    "    df['Female'] = df['Female'].round(decimals=2).abs()\n",
    "    df = df.rename(columns = {'Male': 'Male_'+column_extention, 'Female': 'Female_'+column_extention})\n",
    "    \n",
    "    return df, ages\n",
    "\n",
    "def rand_dist_age_sex(nIDs, all_sub_data, raw_data, column):\n",
    "    \"\"\"\n",
    "    Computes the eucledian distance in age between  all subjects and a randomly drawn \n",
    "    sub-sample of defined size\n",
    "    \n",
    "    Args: \n",
    "        nIDs (int): number of random samples to draw\n",
    "        all_sub_data (list): true age distribution in the overall sample\n",
    "        raw_data (df): data to compute age distribution on\n",
    "        column (string): column in random sample to compute distance on \"Male\" or \"Female\"\n",
    "    \n",
    "    Returns:\n",
    "        euclid_dist (float): eucledian distance between the random sample and the true data\n",
    "    \"\"\"\n",
    "    ids = raw_data['PROLIFIC_PID'].sample(nIDs)\n",
    "    age_sex,_ = get_age_sex_data(ids, raw_data, '')\n",
    "    age_sex = all_sub_data.join(age_sex, how='outer')\n",
    "    age_sex = age_sex.fillna(0)\n",
    "    euclid_dist = np.linalg.norm(all_sub_data[column+'_good']-age_sex[column+'_'])\n",
    "    return euclid_dist\n",
    "\n",
    "def rand_euclDistance_dist_age(compareIds, true_sample, rawData , column , nSample, col_name_extension):\n",
    "    \"\"\"\n",
    "    Generates distribution of eucledian distances between the age distrubution of a randomly drawn \n",
    "    sample of given size and the true sampele\n",
    "    \n",
    "    Args: \n",
    "        compareIds (list): sub-sample to compare against (either exluced subjects or subjects with low completion rate)\n",
    "        true_sample (df): age distribution in the overall sample\n",
    "        raw_data (df): data to compute age distribution on\n",
    "        nSample (int): number of random samples\n",
    "    \n",
    "    Returns:\n",
    "        rand_dist (df): distribution eucledian distances between age in randomly drawn sub-samples \n",
    "                        and age in thetrue sample \n",
    "    \"\"\"\n",
    "    rand_dist = pd.DataFrame()\n",
    "    rand_dist['rand_eucl_dist'+ col_name_extension] = []\n",
    "    for i in range(0,nSample):\n",
    "        rand_dist.loc[i,'rand_eucl_dist'+ col_name_extension] = rand_dist_age_sex(len(compareIds), true_sample, rawData, column)\n",
    "        \n",
    "    return rand_dist\n",
    "\n",
    "\n",
    "def poli_affil(list_of_ids, survey_data, col_name_extention):\n",
    "    ids = [value for value in list_of_ids if value in list(survey_data['PROLIFIC_PID'])]\n",
    "    survey_data = survey_data.set_index('PROLIFIC_PID')\n",
    "    survey = survey_data[['DemM6', 'DemM7']].loc[ids].astype(str)\n",
    "    party_col_name = 'party'+ col_name_extention\n",
    "    lean_col_name = 'polit. lean.'+col_name_extention \n",
    "    survey = survey.rename(columns = {'DemM6': party_col_name,'DemM7': lean_col_name})\n",
    "    party_names={'1.0':'Republican', '2.0':'Democrat', '3.0':'Independant', '4.0':'Other'}\n",
    "    lean_names = {'1.0':'strongly liberal', '2.0':'moderately liberal', '3.0':'slightly liberal',\n",
    "                 '4.0':'neutral', '5.0':'slightly conservative', '6.0':'moderately conservative', '7.0':'strongly conservative'}\n",
    "    survey = survey.replace({party_col_name: party_names}) \n",
    "    survey = survey.replace({lean_col_name: lean_names}) \n",
    "    party = survey[party_col_name].value_counts(normalize=True) * 100\n",
    "    party = party.to_frame()\n",
    "    party[party_col_name] = party[party_col_name].round(decimals=2)\n",
    "    polit_lean = survey[lean_col_name].value_counts(normalize=True) * 100\n",
    "    polit_lean = polit_lean.to_frame()\n",
    "    polit_lean[lean_col_name] = polit_lean[lean_col_name].round(decimals=2)\n",
    "    return party, polit_lean\n",
    "\n",
    "\n",
    "\n",
    "def rand_dist_party(nIDs, all_sub_data, raw_data):\n",
    "    \"\"\"\n",
    "    Computes the eucledian distance in party identity between all subjects and a randomly drawn \n",
    "    sub-sample of defined size\n",
    "    \n",
    "    Args: \n",
    "        nIDs (int): number of random samples to draw\n",
    "        all_sub_data (list): true party identity in the overall sample\n",
    "        raw_data (df): data to compute party identity distribution on\n",
    "    \n",
    "    Returns:\n",
    "        euclid_dist (float): eucledian distance between the random sample and the true data\n",
    "    \"\"\"\n",
    "    ids = raw_data['PROLIFIC_PID'].sample(nIDs)\n",
    "    party,_ = poli_affil(ids, raw_data, '')\n",
    "    party = all_sub_data.join(party, how='outer')\n",
    "    party = party.fillna(0)\n",
    "    euclid_dist = np.linalg.norm(all_sub_data['party_good']-party['party'])\n",
    "    return euclid_dist\n",
    "\n",
    "\n",
    "def rand_euclDistance_dist_party(compareIds, true_sample, rawData, nSample,col_name_extension):\n",
    "    \"\"\"\n",
    "    Generates distribution of eucledian distances between the age distrubution of a randomly drawn \n",
    "    sample of given size and the true sampele\n",
    "    \n",
    "    Args: \n",
    "        compareIds (list): sub-sample to compare against (either exluced subjects or subjects with low completion rate)\n",
    "        true_sample (df): age distribution in the overall sample\n",
    "        raw_data (df): data to compute age distribution on\n",
    "        nSample (int): number of random samples\n",
    "    \n",
    "    Returns:\n",
    "        rand_dist (df): distribution eucledian distances between age in randomly drawn sub-samples \n",
    "                        and age in thetrue sample \n",
    "    \"\"\"\n",
    "    rand_dist = pd.DataFrame()\n",
    "    rand_dist['rand_eucl_dist'+ col_name_extension] = []\n",
    "    for i in range(0,nSample):\n",
    "        rand_dist.loc[i,'rand_eucl_dist'+ col_name_extension] = rand_dist_party(len(compareIds), true_sample, rawData)\n",
    "        \n",
    "    return rand_dist\n",
    "\n",
    "def income_brac(list_of_ids, survey_data, col_name_extention):\n",
    "    ids = [value for value in list_of_ids if value in list(survey_data['PROLIFIC_PID'])]\n",
    "    survey_data = survey_data.set_index('PROLIFIC_PID')\n",
    "    survey = survey_data[['DemW18_R1']].loc[ids].astype(str)\n",
    "    income_col_name = 'income'+ col_name_extention\n",
    "    survey = survey.rename(columns = {'DemW18_R1': income_col_name})\n",
    "    income_names = {'1.0': 'Less than $249', '2.0' : '$250 - $499', '3.0': '$500 - $999', '4.0':'$1000 -$1499', '5.0':'$1500 - $2999', '6.0':'more than $3000', '7.0' :'Dont know'}\n",
    "    survey = survey.replace({income_col_name: income_names}) \n",
    "    income = survey[income_col_name].value_counts(normalize=True) * 100\n",
    "    income = income.to_frame()\n",
    "    income[income_col_name] = income[income_col_name].round(decimals=2)\n",
    "    return income\n",
    "\n",
    "\n",
    "\n",
    "def rand_dist_income(nIDs, all_sub_data, raw_data):\n",
    "    \"\"\"\n",
    "    Computes the eucledian distance in party identity between all subjects and a randomly drawn \n",
    "    sub-sample of defined size\n",
    "    \n",
    "    Args: \n",
    "        nIDs (int): number of random samples to draw\n",
    "        all_sub_data (list): true party identity in the overall sample\n",
    "        raw_data (df): data to compute party identity distribution on\n",
    "    \n",
    "    Returns:\n",
    "        euclid_dist (float): eucledian distance between the random sample and the true data\n",
    "    \"\"\"\n",
    "    ids = raw_data['PROLIFIC_PID'].sample(nIDs)\n",
    "    income = income_brac(ids, raw_data, '')\n",
    "    income = all_sub_data.join(income, how='outer')\n",
    "    income = income.fillna(0)\n",
    "    euclid_dist = np.linalg.norm(income['income_good']-income['income'])\n",
    "    return euclid_dist\n",
    "\n",
    "\n",
    "def rand_euclDistance_dist_income(compareIds, true_sample, rawData, nSample,col_name_extension):\n",
    "    \"\"\"\n",
    "    Generates distribution of eucledian distances between the age distrubution of a randomly drawn \n",
    "    sample of given size and the true sampele\n",
    "    \n",
    "    Args: \n",
    "        compareIds (list): sub-sample to compare against (either exluced subjects or subjects with low completion rate)\n",
    "        true_sample (df): age distribution in the overall sample\n",
    "        raw_data (df): data to compute age distribution on\n",
    "        nSample (int): number of random samples\n",
    "    \n",
    "    Returns:\n",
    "        rand_dist (df): distribution eucledian distances between age in randomly drawn sub-samples \n",
    "                        and age in thetrue sample \n",
    "    \"\"\"\n",
    "    rand_dist = pd.DataFrame()\n",
    "    rand_dist['rand_eucl_dist'+col_name_extension] = []\n",
    "    for i in range(0,nSample):\n",
    "        rand_dist.loc[i,'rand_eucl_dist'+col_name_extension] = rand_dist_income(len(compareIds), true_sample, rawData)\n",
    "        \n",
    "    return rand_dist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def highest_education(list_of_ids, survey_data, col_name_extention):\n",
    "    ids = [value for value in list_of_ids if value in list(survey_data['PROLIFIC_PID'])]\n",
    "    survey_data = survey_data.set_index('PROLIFIC_PID')\n",
    "    survey = survey_data[['DemC23']].loc[ids].astype(str)\n",
    "    education_col_name = 'education'+ col_name_extention\n",
    "    survey = survey.rename(columns = {'DemC23': education_col_name})\n",
    "    education_names = {'1.0': 'Some high school', '2.0' : 'High school', '3.0': 'Some college', \n",
    "                    '4.0':'Associate degree', '5.0':'Bachelor degree', '6.0':'Some graduate education', \n",
    "                    '7.0' :'Master degree', '8.0': 'PhD', '9.0': 'Professional degree', '10.0': 'other'}\n",
    "    survey = survey.replace({education_col_name: education_names}) \n",
    "    education = survey[education_col_name].value_counts(normalize=True) * 100\n",
    "    education = education.to_frame()\n",
    "    education[education_col_name] = education[education_col_name].round(decimals=2)\n",
    "    return education\n",
    "\n",
    "\n",
    "\n",
    "def rand_dist_education(nIDs, all_sub_data, raw_data):\n",
    "    \"\"\"\n",
    "    Computes the eucledian distance in party identity between all subjects and a randomly drawn \n",
    "    sub-sample of defined size\n",
    "    \n",
    "    Args: \n",
    "        nIDs (int): number of random samples to draw\n",
    "        all_sub_data (list): true party identity in the overall sample\n",
    "        raw_data (df): data to compute party identity distribution on\n",
    "    \n",
    "    Returns:\n",
    "        euclid_dist (float): eucledian distance between the random sample and the true data\n",
    "    \"\"\"\n",
    "    ids = raw_data['PROLIFIC_PID'].sample(nIDs)\n",
    "    education = highest_education(ids, raw_data, '')\n",
    "    education = all_sub_data.join(education, how='outer')\n",
    "    education = education.fillna(0)\n",
    "    euclid_dist = np.linalg.norm(education['education_good']-education['education'])\n",
    "    return euclid_dist\n",
    "\n",
    "\n",
    "def rand_euclDistance_dist_education(compareIds, true_sample, rawData, nSample,col_name_extension):\n",
    "    \"\"\"\n",
    "    Generates distribution of eucledian distances between the education distrubution of a randomly drawn \n",
    "    sample of given size and the true sampele\n",
    "    \n",
    "    Args: \n",
    "        compareIds (list): sub-sample to compare against (either exluced subjects or subjects with low completion rate)\n",
    "        true_sample (df): age distribution in the overall sample\n",
    "        raw_data (df): data to compute education distribution on\n",
    "        nSample (int): number of random samples\n",
    "    \n",
    "    Returns:\n",
    "        rand_dist (df): distribution eucledian distances between education in randomly drawn sub-samples \n",
    "                        and age in thetrue sample \n",
    "    \"\"\"\n",
    "    rand_dist = pd.DataFrame()\n",
    "    rand_dist['rand_eucl_dist'+col_name_extension] = []\n",
    "    for i in range(0,nSample):\n",
    "        rand_dist.loc[i,'rand_eucl_dist'+col_name_extension] = rand_dist_education(len(compareIds), true_sample, rawData)\n",
    "        \n",
    "    return rand_dist\n",
    "\n",
    "\n",
    "\n",
    "def raceEthnicity(list_of_ids , survey_data , col_name_extention):\n",
    "    \"\"\"\n",
    "    Extract and merge race and ethnicity data \n",
    "    Args:\n",
    "        list_of_ids (list): subejct IDs for which to return race and tehnicity information\n",
    "        survey_data (pd.Dataframe): qualtrics data\n",
    "        col_name_extention (string): column name extention to characterize teh group of subjects in list_of_ids\n",
    "    \"\"\"\n",
    "    ids = [value for value in list_of_ids if value in list(survey_data['PROLIFIC_PID'])]\n",
    "    survey_data = survey_data.set_index('PROLIFIC_PID')\n",
    "    survey = survey_data[['DemC9']].loc[ids].astype(str)\n",
    "    race_col_name = 'raceEth'+ col_name_extention\n",
    "    survey = survey.rename(columns = {'DemC9': race_col_name})\n",
    "    race_names = {'1.0': 'American Indian/Alaska Native', '2.0' : 'Asian', '3.0': 'Native Hawaiian or Other Pacific Islander', \n",
    "                    '4.0':'Black or African American', '5.0':'White', '6.0':'Multiracial', \n",
    "                    '7.0' :'Other', '8.0': 'Prefer not to disclose'}\n",
    "    survey = survey.replace({race_col_name: race_names}) \n",
    "\n",
    "    # add ethnicity to race column\n",
    "    tmp = survey_data[['DemC8']].loc[ids].astype(str)\n",
    "    ethnic_names = {'1.0':  'Hispanic or Latino',  '2.0' : 'Not Hispanic or Latino', '3.0':'Prefer not to disclose'}\n",
    "    tmp = tmp.replace({'DemC8': ethnic_names}) \n",
    "    hisp_idx = tmp.index[tmp['DemC8']=='Hispanic or Latino'].tolist()\n",
    "    survey[race_col_name].loc[hisp_idx]= 'Hispanic or Latino'\n",
    "    raceEth = survey[race_col_name].value_counts(normalize=True) * 100\n",
    "    raceEth = raceEth.to_frame()\n",
    "    raceEth[race_col_name] = raceEth[race_col_name].round(decimals=2)\n",
    "    return raceEth\n",
    "\n",
    "def rand_dist_raceEthnicity(nIDs, all_sub_data, raw_data):\n",
    "    \"\"\"\n",
    "    Computes the eucledian distance in race/ethnicity identity between all subjects and a randomly drawn \n",
    "    sub-sample of defined size\n",
    "    \n",
    "    Args: \n",
    "        nIDs (int): number of random samples to draw\n",
    "        all_sub_data (list): true race/ethnic identity in the overall sample\n",
    "        raw_data (df): data to compute race/ethnic identity distribution on\n",
    "    \n",
    "    Returns:\n",
    "        euclid_dist (float): eucledian distance between the random sample and the true data\n",
    "    \"\"\"\n",
    "    ids = raw_data['PROLIFIC_PID'].sample(nIDs)\n",
    "    raceEth = raceEthnicity(ids, raw_data, '')\n",
    "    raceEth = all_sub_data.join(raceEth, how='outer')\n",
    "    raceEth = raceEth.fillna(0)\n",
    "    euclid_dist = np.linalg.norm(raceEth['raceEth_good']-raceEth['raceEth'])\n",
    "    return euclid_dist\n",
    "\n",
    "\n",
    "def rand_euclDistance_dist_raceEthnicity(compareIds, true_sample, rawData, nSample,col_name_extension):\n",
    "    \"\"\"\n",
    "    Generates distribution of eucledian distances between the education distrubution of a randomly drawn \n",
    "    sample of given size and the true sampele\n",
    "    \n",
    "    Args: \n",
    "        compareIds (list): sub-sample to compare against (either exluced subjects or subjects with low completion rate)\n",
    "        true_sample (df): age distribution in the overall sample\n",
    "        raw_data (df): data to compute education distribution on\n",
    "        nSample (int): number of random samples\n",
    "    \n",
    "    Returns:\n",
    "        rand_dist (df): distribution eucledian distances between education in randomly drawn sub-samples \n",
    "                        and age in thetrue sample \n",
    "    \"\"\"\n",
    "    rand_dist = pd.DataFrame()\n",
    "    rand_dist['rand_eucl_dist'+col_name_extension] = []\n",
    "    for i in range(0,nSample):\n",
    "        rand_dist.loc[i,'rand_eucl_dist'+col_name_extension] = rand_dist_raceEthnicity(len(compareIds), true_sample, rawData)\n",
    "        \n",
    "    return rand_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_sex_butterfly_plot(data, col_label_loss, ax):\n",
    "    \"\"\"\n",
    "    Plots a population age pyramid for core data with overlayed excluded or low completion data\n",
    "    \n",
    "    Args:\n",
    "       list_of_ids (list or array): list of ids to be included\n",
    "        survey_data (pd.DataFrame): dataframe containing survey data for one week\n",
    "        census_data (pd.DataFrame): dataframe containing census information\n",
    "\n",
    "    Returns:\n",
    "        ax: axes of the chart\n",
    "    \"\"\"\n",
    "    bar_plot1 = sns.barplot(x='Female_good', y = data.index, data = data,  lw=0, color='indianred', alpha=0.7,ax=ax)\n",
    "    sns.barplot(x='Male_good', y=data.index, data = data,  lw=0, color='royalblue', alpha=0.7,ax=ax)\n",
    "    sns.barplot(x='Female_' + col_label_loss, y=data.index,data = data, lw=0, color='indianred', alpha=0.4,ax=ax)\n",
    "    sns.barplot(x='Male_'+col_label_loss, y=data.index, data = data, lw=0, color='royalblue', alpha=0.4,ax=ax).invert_yaxis()\n",
    "    \n",
    "    ax.set_ylabel('ages')\n",
    "    ax.set_xlabel('percent')\n",
    "    max_val = abs(age_sex_plot).max().max() * 1.05\n",
    "    ax.set_xlim((-max_val, max_val))\n",
    "    custom_labels = ['female core', 'male core', 'female '+ col_label_loss, 'male '+ col_label_loss]\n",
    "    custom_lines = [Line2D([0], [0], color='indianred', alpha=0.7, linewidth=25),\n",
    "                Line2D([0], [0], color='royalblue', alpha=0.7, linewidth=25), \n",
    "                Line2D([0], [0], color='indianred', alpha=0.4, linewidth=25),\n",
    "                Line2D([0], [0], color='royalblue', alpha=0.4, linewidth=25)]\n",
    "    ax.legend(custom_lines, custom_labels, loc='best')\n",
    "    ax.set_xticklabels(np.round(abs(bar_plot1.get_xticks()), 4))\n",
    "    ax.set_yticklabels(data.index)\n",
    "\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def hist_sampledEuclidDist(sampled_dist, true_dist, ax, title, linewidth):\n",
    "    \"\"\"\n",
    "    Plots histogram of sampled eucledian distance distribution,2.5th and 97.5th percentile\n",
    "    and true eucledian distance between an attrition sample and the good sample of subjects.\n",
    "    \n",
    "    Args:\n",
    "        sampled_dist (pd.DataFrame): distribution of eucledian distances between random subsamples and good subjects\n",
    "        true_dist (list): eucledian distance between attrition sample and good sample\n",
    "        ax (axis handle): axis handle axes of the chart\n",
    "        title (string): plot title\n",
    "\n",
    "    Returns:\n",
    "        ax: axes of the chart\n",
    "    \"\"\"\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    p2_5, p50, p97_5, = np.percentile(sampled_dist.iloc[:,0], [2.5, 50, 97.5])\n",
    "    ax = sampled_dist.plot.hist(bins= 100, ax=ax)\n",
    "    ax.axvline(true_dist, color='r', linewidth=linewidth)\n",
    "    ax.axvline(p2_5, color='black', linestyle = ':', linewidth=linewidth)\n",
    "    ax.axvline(p97_5, color='black', linestyle = ':', linewidth=linewidth)\n",
    "    ax.set_xlabel('eucl. dist.')\n",
    "    ax.set_ylabel('frequency')\n",
    "    ax.set_title(title)\n",
    "    ax.get_legend().remove()\n",
    "    ax.set_ylim([0, 400])\n",
    "    tick_spacing = 5\n",
    "    ax.set_yticks(np.arange(0, 400+1, 200.0))\n",
    "    \n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(tick_spacing))\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def nested_pie(data, col_outer, col_inner, ax, leg_title, colors):\n",
    "    \"\"\"\n",
    "    Plots a nested pie chart\n",
    "    \n",
    "    Args:\n",
    "        data (pd.Dataframe): data to plot as pie charts\n",
    "        col_outer (string): column name in data for outer pie\n",
    "        col_inner (string): column name in data for inner pie\n",
    "        ax (axes): axes to plot the pie in\n",
    "        title (string): legend title\n",
    "\n",
    "    Returns:\n",
    "        ax: axes of the pie chart\n",
    "    \"\"\"\n",
    "\n",
    "    exp_colors_outer = colors\n",
    "    \n",
    "    exp_colors_inner= []\n",
    "    for i_col, _ in enumerate(exp_colors_outer):\n",
    "        exp_colors_inner.append(sns.set_hls_values(matplotlib.colors.to_rgb(exp_colors_outer[i_col]),l=0.7))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # outer pie\n",
    "    mypie, mylables,prct = ax.pie(data[col_outer], radius = 0.8, autopct='%1.1f%%',pctdistance=1.1, colors = exp_colors_outer)\n",
    "    plt.setp(mypie,  edgecolor='white')\n",
    "    plt.setp(mylables, color = 'black')\n",
    "    plt.setp(prct, color = 'black')\n",
    "    plt.setp(mypie,  edgecolor='white')\n",
    "    plt.setp(mylables, color = 'black')\n",
    "    plt.setp(prct, color = 'black')\n",
    "    # inner pie\n",
    "    mypieInner,mylablesInner, prctInner =  ax.pie(data[col_inner], radius = 0.5, colors = exp_colors_inner,autopct='%1.1f%%',pctdistance=0.7)\n",
    "    plt.setp(prctInner, color = 'black')\n",
    "    plt.setp(mypieInner,  edgecolor='black')\n",
    "    # legend\n",
    "    ax.legend(data.index,loc = 'upper right', title = leg_title)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# post-hoc comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trusch/opt/anaconda3/envs/covid_base/lib/python3.8/site-packages/statsmodels/stats/weightstats.py:790: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  zstat = value / std\n"
     ]
    }
   ],
   "source": [
    "core_data_fem_age = core_data.loc[core_data.DemC5== '2.0','prlfc_dem_age' ]\n",
    "excluded_data_fem_age = excluded_data.loc[excluded_data.DemC5== '2.0','prlfc_dem_age' ]\n",
    "lowCompl_data_fem_age = lowCompl_data.loc[lowCompl_data.DemC5== '2.0','prlfc_dem_age' ]\n",
    "\n",
    "core_data_male_age = core_data.loc[core_data.DemC5== '1.0','prlfc_dem_age' ]\n",
    "excluded_data_male_age = excluded_data.loc[excluded_data.DemC5== '1.0','prlfc_dem_age' ]\n",
    "lowCompl_data_male_age = lowCompl_data.loc[lowCompl_data.DemC5== '1.0','prlfc_dem_age' ]\n",
    "\n",
    "\n",
    "bins=  [i*5 for i in range(3, 21)]\n",
    "age_count = pd.DataFrame(columns=['core','excl'])\n",
    "age_count['bins'] = bins\n",
    "age_count = age_count.set_index('bins')\n",
    "age_count_fem = age_count.copy()\n",
    "age_count_male = age_count.copy()\n",
    "\n",
    "\n",
    "for i_bin in bins:\n",
    "        \n",
    "    age_count_fem.loc[i_bin, 'core'] = sum((core_data_fem_age >= i_bin) & (core_data_fem_age < i_bin+5))\n",
    "    age_count_fem.loc[i_bin, 'excl'] = sum((excluded_data_fem_age >= i_bin) & (excluded_data_fem_age < i_bin+5))\n",
    "    age_count_fem.loc[i_bin, 'low_compl'] = sum((lowCompl_data_fem_age >= i_bin) & (lowCompl_data_fem_age < i_bin+5))\n",
    "\n",
    "\n",
    "    age_count_male.loc[i_bin, 'core'] = sum((core_data_male_age >= i_bin) & (core_data_male_age < i_bin+5))\n",
    "    age_count_male.loc[i_bin, 'excl'] = sum((excluded_data_male_age >= i_bin) & (excluded_data_male_age < i_bin+5))\n",
    "    age_count_male.loc[i_bin, 'low_compl'] = sum((lowCompl_data_male_age >= i_bin) & (lowCompl_data_male_age < i_bin+5))\n",
    "\n",
    "age_count_male = age_count_male.drop([85, 90, 95, 100])   \n",
    "age_count_fem = age_count_fem.drop([85, 90, 95, 100])    \n",
    "\n",
    "\n",
    "    \n",
    "age_prop_fem = pd.DataFrame()\n",
    "age_prop_fem['core'] = age_count_fem.core.div(sum(age_count_fem.core))\n",
    "age_prop_fem['excl'] = age_count_fem.excl.div(sum(age_count_fem.excl))\n",
    "age_prop_fem['low_compl'] = age_count_fem.low_compl.div(sum(age_count_fem.low_compl))\n",
    "age_prop_fem['abs diff. core - excl.'] = abs(age_prop_fem['core'] - age_prop_fem['excl'])\n",
    "age_prop_fem['abs diff. core - low compl.'] = abs(age_prop_fem['core'] - age_prop_fem['low_compl'])\n",
    "\n",
    "age_prop_male = pd.DataFrame()\n",
    "age_prop_male['core'] = age_count_male.core.div(sum(age_count_male.core))\n",
    "age_prop_male['excl'] = age_count_male.excl.div(sum(age_count_male.excl))\n",
    "age_prop_male['low_compl'] = age_count_male.low_compl.div(sum(age_count_male.low_compl))\n",
    "age_prop_male['abs diff. core - excl.'] = abs(age_prop_male['core'] - age_prop_male['excl'])\n",
    "age_prop_male['abs diff. core - low compl.'] = abs(age_prop_male['core'] - age_prop_male['low_compl'])\n",
    "\n",
    "\n",
    "for i in age_count_male.index:\n",
    "    count = np.array([age_count_male.loc[i, 'core'], age_count_male.loc[i, 'excl']])\n",
    "    nobs = np.array([age_count_male.core.sum(), age_count_male.excl.sum()])\n",
    "    stat, pval = proportions_ztest(count, nobs)\n",
    "    age_prop_male.loc[i, 'z-val (core/excl.)'] = stat\n",
    "    age_prop_male.loc[i, 'p-val (core/excl.)'] = pval\n",
    "    \n",
    "for i in age_count_fem.index:\n",
    "    count = np.array([age_count_fem.loc[i, 'core'], age_count_fem.loc[i, 'excl']])\n",
    "    nobs = np.array([age_count_fem.core.sum(), age_count_fem.excl.sum()])\n",
    "    stat, pval = proportions_ztest(count, nobs)\n",
    "    age_prop_fem.loc[i, 'z-val (core/excl.)'] = stat\n",
    "    age_prop_fem.loc[i, 'p-val (core/excl.)'] = pval\n",
    "    \n",
    "for i in age_count_male.index:\n",
    "    count = np.array([age_count_male.loc[i, 'core'], age_count_male.loc[i, 'low_compl']])\n",
    "    nobs = np.array([age_count_male.core.sum(), age_count_male.low_compl.sum()])\n",
    "    stat, pval = proportions_ztest(count, nobs)\n",
    "    age_prop_male.loc[i, 'z-val (core/low compl.)'] = stat\n",
    "    age_prop_male.loc[i, 'p-val (core/low compl.)'] = pval\n",
    "    \n",
    "for i in age_count_fem.index:\n",
    "    count = np.array([age_count_fem.loc[i, 'core'], age_count_fem.loc[i, 'low_compl']])\n",
    "    nobs = np.array([age_count_fem.core.sum(), age_count_fem.low_compl.sum()])\n",
    "    stat, pval = proportions_ztest(count, nobs)\n",
    "    age_prop_fem.loc[i, 'z-val (core/low compl.)'] = stat\n",
    "    age_prop_fem.loc[i, 'p-val (core/low compl.)'] = pval\n",
    "\n",
    "age_prop_fem = age_prop_fem.astype(float).round(decimals=3)\n",
    "\n",
    "age_prop_male = age_prop_male.astype(float).round(decimals=3)\n",
    "\n",
    "labels = [str(i*5) + '-' + str((i+1)*5-1) for i in range(3,17)]\n",
    "labels[0] = '18-19'\n",
    "\n",
    "age_prop_male['age'] = labels\n",
    "age_prop_fem['age'] = labels\n",
    "\n",
    "age_prop_fem.set_index('age', drop = True, inplace = True)\n",
    "age_prop_male.set_index('age', drop = True, inplace = True)\n",
    "\n",
    "    \n",
    "age_prop_male.to_csv('attrition_male_age_bracket_proportion.csv')\n",
    "age_prop_fem.to_csv('attrition_female_age_bracket_proportion.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>core</th>\n",
       "      <th>excl.</th>\n",
       "      <th>low compl.</th>\n",
       "      <th>abs diff. core - excl.</th>\n",
       "      <th>abs diff. core - low compl.</th>\n",
       "      <th>z-val (core/excl.)</th>\n",
       "      <th>p-val (core/excl.)</th>\n",
       "      <th>z-val (core/low compl.)</th>\n",
       "      <th>p-val (core/low compl.)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PhD</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>0.758</td>\n",
       "      <td>1.585</td>\n",
       "      <td>0.113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Master degree</th>\n",
       "      <td>0.138</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>2.099</td>\n",
       "      <td>0.036</td>\n",
       "      <td>2.606</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Some graduate education</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-1.222</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-1.107</td>\n",
       "      <td>0.268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bachelor degree</th>\n",
       "      <td>0.332</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.836</td>\n",
       "      <td>1.533</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Professional degree</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Associate degree</th>\n",
       "      <td>0.109</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.768</td>\n",
       "      <td>0.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Some college</th>\n",
       "      <td>0.232</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-3.618</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>High school</th>\n",
       "      <td>0.111</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-1.045</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Some high school</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.722</td>\n",
       "      <td>0.470</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-1.326</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>0.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          core  excl.  low compl.  abs diff. core - excl.  \\\n",
       "PhD                      0.015  0.018       0.005                   0.003   \n",
       "Master degree            0.138  0.086       0.088                   0.052   \n",
       "Some graduate education  0.033  0.050       0.045                   0.017   \n",
       "Bachelor degree          0.332  0.339       0.291                   0.007   \n",
       "Professional degree      0.020  0.023       0.018                   0.002   \n",
       "Associate degree         0.109  0.090       0.123                   0.018   \n",
       "Some college             0.232  0.240       0.323                   0.008   \n",
       "High school              0.111  0.136       0.095                   0.024   \n",
       "Some high school         0.008  0.014       0.010                   0.005   \n",
       "other                    0.001  0.005       0.003                   0.004   \n",
       "\n",
       "                         abs diff. core - low compl.  z-val (core/excl.)  \\\n",
       "PhD                                            0.010              -0.308   \n",
       "Master degree                                  0.050               2.099   \n",
       "Some graduate education                        0.012              -1.222   \n",
       "Bachelor degree                                0.041              -0.207   \n",
       "Professional degree                            0.003              -0.214   \n",
       "Associate degree                               0.014               0.809   \n",
       "Some college                                   0.091              -0.254   \n",
       "High school                                    0.016              -1.045   \n",
       "Some high school                               0.002              -0.722   \n",
       "other                                          0.002              -1.326   \n",
       "\n",
       "                         p-val (core/excl.)  z-val (core/low compl.)  \\\n",
       "PhD                                   0.758                    1.585   \n",
       "Master degree                         0.036                    2.606   \n",
       "Some graduate education               0.222                   -1.107   \n",
       "Bachelor degree                       0.836                    1.533   \n",
       "Professional degree                   0.831                    0.354   \n",
       "Associate degree                      0.418                   -0.768   \n",
       "Some college                          0.800                   -3.618   \n",
       "High school                           0.296                    0.896   \n",
       "Some high school                      0.470                   -0.281   \n",
       "other                                 0.185                   -0.803   \n",
       "\n",
       "                         p-val (core/low compl.)  \n",
       "PhD                                        0.113  \n",
       "Master degree                              0.009  \n",
       "Some graduate education                    0.268  \n",
       "Bachelor degree                            0.125  \n",
       "Professional degree                        0.723  \n",
       "Associate degree                           0.442  \n",
       "Some college                               0.000  \n",
       "High school                                0.370  \n",
       "Some high school                           0.779  \n",
       "other                                      0.422  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "education_names = {'1.0': 'Some high school', '2.0' : 'High school', '3.0': 'Some college', \n",
    "                    '4.0':'Associate degree', '5.0':'Bachelor degree', '6.0':'Some graduate education', \n",
    "                    '7.0' :'Master degree', '8.0': 'PhD', '9.0': 'Professional degree', '10.0': 'other'}\n",
    "core_data.DemC23 = core_data.DemC23.replace(education_names)\n",
    "excluded_data.DemC23 = excluded_data.DemC23.replace(education_names)\n",
    "lowCompl_data.DemC23 = lowCompl_data.DemC23.replace(education_names)\n",
    "\n",
    "core_ed = pd.DataFrame(core_data['DemC23'].value_counts()/ core_data['DemC23'].count())\n",
    "core_ed.rename(columns = {'DemC23': 'core'}, inplace = True)\n",
    "core_ed_count = pd.DataFrame(core_data['DemC23'].value_counts())\n",
    "core_ed_count.rename(columns = {'DemC23': 'core'}, inplace = True)\n",
    "\n",
    "excl_ed = pd.DataFrame(excluded_data['DemC23'].value_counts()/ excluded_data['DemC23'].count())\n",
    "excl_ed.rename(columns = {'DemC23': 'excl.'}, inplace = True)\n",
    "excl_ed_count = pd.DataFrame(excluded_data['DemC23'].value_counts())\n",
    "excl_ed_count.rename(columns = {'DemC23': 'excl.'}, inplace = True)\n",
    "                             \n",
    "lowcompl_ed = pd.DataFrame(lowCompl_data['DemC23'].value_counts()/ lowCompl_data['DemC23'].count())\n",
    "lowcompl_ed.rename(columns = {'DemC23': 'low compl.'}, inplace = True)\n",
    "lowcompl_ed_count = pd.DataFrame(lowCompl_data['DemC23'].value_counts())\n",
    "lowcompl_ed_count.rename(columns = {'DemC23': 'low compl.'}, inplace = True)\n",
    "\n",
    "education_count = core_ed_count.merge(excl_ed_count, left_index = True, right_index=True, how = 'outer')\n",
    "education_count  = education_count.merge(lowcompl_ed_count, left_index = True, right_index=True, how = 'outer')\n",
    "education_count = education_count.reindex(['PhD', 'Master degree', 'Some graduate education',\n",
    "                                         'Bachelor degree', 'Professional degree','Associate degree', \n",
    "                                         'Some college', 'High school', 'Some high school', 'other'])\n",
    "                                 \n",
    "education_prop = core_ed.merge(excl_ed, left_index = True, right_index=True, how = 'outer')\n",
    "education_prop  = education_prop.merge(lowcompl_ed, left_index = True, right_index=True, how = 'outer')\n",
    "education_prop['abs diff. core - excl.'] = abs(education_prop['core'] - education_prop['excl.'])\n",
    "education_prop['abs diff. core - low compl.'] = abs(education_prop['core'] - education_prop['low compl.'])\n",
    "education_prop = education_prop.reindex(['PhD', 'Master degree', 'Some graduate education',\n",
    "                                         'Bachelor degree', 'Professional degree','Associate degree', \n",
    "                                         'Some college', 'High school', 'Some high school', 'other'])\n",
    "\n",
    "education_count.to_csv('attrition_education_bracket_count.csv')\n",
    "\n",
    "for i in education_count.index:\n",
    "    count = np.array([education_count.loc[i, 'core'], education_count.loc[i, 'excl.']])\n",
    "    nobs = np.array([education_count.core.sum(), education_count['excl.'].sum()])\n",
    "    stat, pval = proportions_ztest(count, nobs)\n",
    "    education_prop.loc[i, 'z-val (core/excl.)'] = stat\n",
    "    education_prop.loc[i, 'p-val (core/excl.)'] = pval\n",
    "    \n",
    "    count = np.array([education_count.loc[i, 'core'], education_count.loc[i, 'low compl.']])\n",
    "    nobs = np.array([education_count.core.sum(), education_count['low compl.'].sum()])\n",
    "    stat, pval = proportions_ztest(count, nobs)\n",
    "    education_prop.loc[i, 'z-val (core/low compl.)'] = stat\n",
    "    education_prop.loc[i, 'p-val (core/low compl.)'] = pval\n",
    "    \n",
    "education_prop = education_prop.astype(float).round(decimals=3)\n",
    "education_prop.to_csv('attrition_education_bracket_proportion.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>core</th>\n",
       "      <th>excl.</th>\n",
       "      <th>low compl.</th>\n",
       "      <th>abs diff. core - excl.</th>\n",
       "      <th>abs diff. core - low compl.</th>\n",
       "      <th>z-val (core/excl.)</th>\n",
       "      <th>p-val (core/excl.)</th>\n",
       "      <th>z-val (core/low compl.)</th>\n",
       "      <th>p-val (core/low compl.)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>more than $3000</th>\n",
       "      <td>0.034</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.601</td>\n",
       "      <td>1.400</td>\n",
       "      <td>0.162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$1500 - $2999</th>\n",
       "      <td>0.095</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.012</td>\n",
       "      <td>1.960</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$1000 -$1499</th>\n",
       "      <td>0.128</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.802</td>\n",
       "      <td>2.032</td>\n",
       "      <td>0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$500 - $999</th>\n",
       "      <td>0.169</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.788</td>\n",
       "      <td>-0.748</td>\n",
       "      <td>0.455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$250 - $499</th>\n",
       "      <td>0.150</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.869</td>\n",
       "      <td>3.401</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Less than $249</th>\n",
       "      <td>0.393</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-1.142</td>\n",
       "      <td>0.253</td>\n",
       "      <td>-4.206</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dont know</th>\n",
       "      <td>0.031</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  core  excl.  low compl.  abs diff. core - excl.  \\\n",
       "more than $3000  0.034  0.027       0.020                   0.007   \n",
       "$1500 - $2999    0.095  0.054       0.083                   0.041   \n",
       "$1000 -$1499     0.128  0.122       0.090                   0.006   \n",
       "$500 - $999      0.169  0.176       0.185                   0.007   \n",
       "$250 - $499      0.150  0.154       0.083                   0.004   \n",
       "Less than $249   0.393  0.434       0.514                   0.041   \n",
       "Dont know        0.031  0.032       0.025                   0.001   \n",
       "\n",
       "                 abs diff. core - low compl.  z-val (core/excl.)  \\\n",
       "more than $3000                        0.014               0.523   \n",
       "$1500 - $2999                          0.012               1.960   \n",
       "$1000 -$1499                           0.038               0.250   \n",
       "$500 - $999                            0.016              -0.268   \n",
       "$250 - $499                            0.067              -0.165   \n",
       "Less than $249                         0.120              -1.142   \n",
       "Dont know                              0.006              -0.086   \n",
       "\n",
       "                 p-val (core/excl.)  z-val (core/low compl.)  \\\n",
       "more than $3000               0.601                    1.400   \n",
       "$1500 - $2999                 0.050                    0.744   \n",
       "$1000 -$1499                  0.802                    2.032   \n",
       "$500 - $999                   0.788                   -0.748   \n",
       "$250 - $499                   0.869                    3.401   \n",
       "Less than $249                0.253                   -4.206   \n",
       "Dont know                     0.932                    0.566   \n",
       "\n",
       "                 p-val (core/low compl.)  \n",
       "more than $3000                    0.162  \n",
       "$1500 - $2999                      0.457  \n",
       "$1000 -$1499                       0.042  \n",
       "$500 - $999                        0.455  \n",
       "$250 - $499                        0.001  \n",
       "Less than $249                     0.000  \n",
       "Dont know                          0.571  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_names = {'1.0': 'Less than $249', '2.0' : '$250 - $499', '3.0': '$500 - $999', '4.0':'$1000 -$1499', '5.0':'$1500 - $2999', '6.0':'more than $3000', '7.0' :'Dont know'}\n",
    "\n",
    "\n",
    "core_data.DemW18_R1 = core_data.DemW18_R1.replace(income_names)\n",
    "excluded_data.DemW18_R1 = excluded_data.DemW18_R1.replace(income_names)\n",
    "lowCompl_data.DemW18_R1 = lowCompl_data.DemW18_R1.replace(income_names)\n",
    "\n",
    "\n",
    "income_prop = pd.DataFrame()\n",
    "income_count= pd.DataFrame()\n",
    "core_income = pd.DataFrame(core_data['DemW18_R1'].value_counts()/ core_data['DemW18_R1'].count())\n",
    "core_income.rename(columns = {'DemW18_R1': 'core'}, inplace = True)\n",
    "core_income_count = pd.DataFrame(core_data['DemW18_R1'].value_counts())\n",
    "core_income_count.rename(columns = {'DemW18_R1': 'core'}, inplace = True)\n",
    "\n",
    "excl_income = pd.DataFrame(excluded_data['DemW18_R1'].value_counts()/ excluded_data['DemW18_R1'].count())\n",
    "excl_income.rename(columns = {'DemW18_R1': 'excl.'}, inplace = True)\n",
    "excl_income_count = pd.DataFrame(excluded_data['DemW18_R1'].value_counts())\n",
    "excl_income_count.rename(columns = {'DemW18_R1': 'excl.'}, inplace = True)\n",
    "                             \n",
    "lowcompl_income = pd.DataFrame(lowCompl_data[ 'DemW18_R1'].value_counts()/ lowCompl_data['DemW18_R1'].count())\n",
    "lowcompl_income.rename(columns = {'DemW18_R1': 'low compl.'}, inplace = True)\n",
    "lowcompl_income_count = pd.DataFrame(lowCompl_data['DemW18_R1'].value_counts())\n",
    "lowcompl_income_count.rename(columns = {'DemW18_R1': 'low compl.'}, inplace = True)\n",
    "\n",
    "income_count = core_income_count.merge(excl_income_count, left_index = True, right_index=True, how = 'outer')\n",
    "income_count  = income_count.merge(lowcompl_income_count, left_index = True, right_index=True, how = 'outer')\n",
    "income_count = income_count.reindex(['more than $3000', '$1500 - $2999',  \n",
    "                                   '$1000 -$1499','$500 - $999', \n",
    "                                   '$250 - $499', 'Less than $249', 'Dont know'])\n",
    "                                 \n",
    "income_prop = core_income.merge(excl_income, left_index = True, right_index=True, how = 'outer')\n",
    "income_prop  = income_prop.merge(lowcompl_income, left_index = True, right_index=True, how = 'outer')\n",
    "income_prop['abs diff. core - excl.'] = abs(income_prop['core'] - income_prop['excl.'])\n",
    "income_prop['abs diff. core - low compl.'] = abs(income_prop['core'] - income_prop['low compl.'])\n",
    "income_prop = income_prop.reindex(['more than $3000', '$1500 - $2999',  \n",
    "                                   '$1000 -$1499','$500 - $999', \n",
    "                                   '$250 - $499', 'Less than $249', 'Dont know'])\n",
    "\n",
    "income_count.to_csv('attrition_income_bracket_count.csv')\n",
    "\n",
    "for i in income_count.index:\n",
    "    count = np.array([income_count.loc[i, 'core'], income_count.loc[i, 'excl.']])\n",
    "    nobs = np.array([income_count.core.sum(), income_count['excl.'].sum()])\n",
    "    stat, pval = proportions_ztest(count, nobs)\n",
    "    income_prop.loc[i, 'z-val (core/excl.)'] = stat\n",
    "    income_prop.loc[i, 'p-val (core/excl.)'] = pval\n",
    "    \n",
    "    count = np.array([income_count.loc[i, 'core'], income_count.loc[i, 'low compl.']])\n",
    "    nobs = np.array([income_count.core.sum(), income_count['low compl.'].sum()])\n",
    "    stat, pval = proportions_ztest(count, nobs)\n",
    "    income_prop.loc[i, 'z-val (core/low compl.)'] = stat\n",
    "    income_prop.loc[i, 'p-val (core/low compl.)'] = pval\n",
    "\n",
    "income_prop = income_prop.astype(float).round(decimals=3)\n",
    "income_prop.to_csv('attrition_income_bracket_proportion.csv')\n",
    "\n",
    "\n",
    "income_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "race/ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>core</th>\n",
       "      <th>excl.</th>\n",
       "      <th>low compl.</th>\n",
       "      <th>abs diff. core - excl.</th>\n",
       "      <th>abs diff. core - low compl.</th>\n",
       "      <th>z-val (core/excl.)</th>\n",
       "      <th>p-val (core/excl.)</th>\n",
       "      <th>z-val (core/low compl.)</th>\n",
       "      <th>p-val (core/low compl.)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>American Indian/Alaska Native</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-3.250</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-1.649</td>\n",
       "      <td>0.099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <td>0.101</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-2.873</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Black or African American</th>\n",
       "      <td>0.071</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.357</td>\n",
       "      <td>-0.747</td>\n",
       "      <td>0.455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hispanic/Latinx</th>\n",
       "      <td>0.094</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.016</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.298</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>0.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multiracial</th>\n",
       "      <td>0.031</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.478</td>\n",
       "      <td>-1.286</td>\n",
       "      <td>0.198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Native Hawaiian or Other Pacific Islander</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-1.326</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>0.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-1.687</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prefer not to disclose</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-2.658</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White</th>\n",
       "      <td>0.692</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.031</td>\n",
       "      <td>2.254</td>\n",
       "      <td>0.024</td>\n",
       "      <td>1.144</td>\n",
       "      <td>0.253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            core  excl.  low compl.  \\\n",
       "American Indian/Alaska Native              0.001  0.014       0.005   \n",
       "Asian                                      0.101  0.167       0.088   \n",
       "Black or African American                  0.071  0.054       0.083   \n",
       "Hispanic/Latinx                            0.094  0.072       0.110   \n",
       "Multiracial                                0.031  0.041       0.045   \n",
       "Native Hawaiian or Other Pacific Islander  0.001  0.005       0.003   \n",
       "Other                                      0.004  0.014       0.003   \n",
       "Prefer not to disclose                     0.003  0.018       0.003   \n",
       "White                                      0.692  0.615       0.662   \n",
       "\n",
       "                                           abs diff. core - excl.  \\\n",
       "American Indian/Alaska Native                               0.013   \n",
       "Asian                                                       0.066   \n",
       "Black or African American                                   0.017   \n",
       "Hispanic/Latinx                                             0.022   \n",
       "Multiracial                                                 0.009   \n",
       "Native Hawaiian or Other Pacific Islander                   0.004   \n",
       "Other                                                       0.009   \n",
       "Prefer not to disclose                                      0.015   \n",
       "White                                                       0.077   \n",
       "\n",
       "                                           abs diff. core - low compl.  \\\n",
       "American Indian/Alaska Native                                    0.004   \n",
       "Asian                                                            0.013   \n",
       "Black or African American                                        0.011   \n",
       "Hispanic/Latinx                                                  0.016   \n",
       "Multiracial                                                      0.014   \n",
       "Native Hawaiian or Other Pacific Islander                        0.002   \n",
       "Other                                                            0.002   \n",
       "Prefer not to disclose                                           0.001   \n",
       "White                                                            0.031   \n",
       "\n",
       "                                           z-val (core/excl.)  \\\n",
       "American Indian/Alaska Native                          -3.250   \n",
       "Asian                                                  -2.873   \n",
       "Black or African American                               0.921   \n",
       "Hispanic/Latinx                                         1.040   \n",
       "Multiracial                                            -0.710   \n",
       "Native Hawaiian or Other Pacific Islander              -1.326   \n",
       "Other                                                  -1.687   \n",
       "Prefer not to disclose                                 -2.658   \n",
       "White                                                   2.254   \n",
       "\n",
       "                                           p-val (core/excl.)  \\\n",
       "American Indian/Alaska Native                           0.001   \n",
       "Asian                                                   0.004   \n",
       "Black or African American                               0.357   \n",
       "Hispanic/Latinx                                         0.298   \n",
       "Multiracial                                             0.478   \n",
       "Native Hawaiian or Other Pacific Islander               0.185   \n",
       "Other                                                   0.092   \n",
       "Prefer not to disclose                                  0.008   \n",
       "White                                                   0.024   \n",
       "\n",
       "                                           z-val (core/low compl.)  \\\n",
       "American Indian/Alaska Native                               -1.649   \n",
       "Asian                                                        0.778   \n",
       "Black or African American                                   -0.747   \n",
       "Hispanic/Latinx                                             -0.926   \n",
       "Multiracial                                                 -1.286   \n",
       "Native Hawaiian or Other Pacific Islander                   -0.803   \n",
       "Other                                                        0.488   \n",
       "Prefer not to disclose                                       0.274   \n",
       "White                                                        1.144   \n",
       "\n",
       "                                           p-val (core/low compl.)  \n",
       "American Indian/Alaska Native                                0.099  \n",
       "Asian                                                        0.436  \n",
       "Black or African American                                    0.455  \n",
       "Hispanic/Latinx                                              0.355  \n",
       "Multiracial                                                  0.198  \n",
       "Native Hawaiian or Other Pacific Islander                    0.422  \n",
       "Other                                                        0.625  \n",
       "Prefer not to disclose                                       0.784  \n",
       "White                                                        0.253  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_names = {'1.0': 'American Indian/Alaska Native', '2.0' : 'Asian', '3.0': 'Native Hawaiian or Other Pacific Islander', \n",
    "              '4.0':'Black or African American', '5.0':'White', '6.0':'Multiracial', '7.0' :'Other',\n",
    "              '8.0' :'Prefer not to disclose'}\n",
    "core_data.DemC9 = core_data.DemC9.replace(race_names)\n",
    "excluded_data.DemC9 = excluded_data.DemC9.replace(race_names)\n",
    "lowCompl_data.DemC9 = lowCompl_data.DemC9.replace(race_names)\n",
    "\n",
    "\n",
    "core_data.loc[core_data.DemC8 == '1.0', 'DemC9'] = 'Hispanic/Latinx'\n",
    "excluded_data.loc[excluded_data.DemC8 == '1.0', 'DemC9'] = 'Hispanic/Latinx'\n",
    "lowCompl_data.loc[lowCompl_data.DemC8 == '1.0', 'DemC9'] = 'Hispanic/Latinx'\n",
    "\n",
    "\n",
    "raceEth_prop = pd.DataFrame()\n",
    "raceEth_count= pd.DataFrame()\n",
    "core_raceEth = pd.DataFrame(core_data[ 'DemC9'].value_counts()/ core_data[ 'DemW18_R1'].count())\n",
    "core_raceEth.rename(columns = {'DemC9': 'core'}, inplace = True)\n",
    "core_raceEth_count = pd.DataFrame(core_data[ 'DemC9'].value_counts())\n",
    "core_raceEth_count.rename(columns = {'DemC9': 'core'}, inplace = True)\n",
    "\n",
    "excl_raceEth = pd.DataFrame(excluded_data[ 'DemC9'].value_counts()/ excluded_data[ 'DemW18_R1'].count())\n",
    "excl_raceEth.rename(columns = {'DemC9': 'excl.'}, inplace = True)\n",
    "excl_raceEth_count = pd.DataFrame(excluded_data[ 'DemC9'].value_counts())\n",
    "excl_raceEth_count.rename(columns = {'DemC9': 'excl.'}, inplace = True)\n",
    "                             \n",
    "lowcompl_raceEth = pd.DataFrame(lowCompl_data[ 'DemC9'].value_counts()/ lowCompl_data[ 'DemW18_R1'].count())\n",
    "lowcompl_raceEth.rename(columns = {'DemC9': 'low compl.'}, inplace = True)\n",
    "lowcompl_raceEth_count = pd.DataFrame(lowCompl_data[ 'DemC9'].value_counts())\n",
    "lowcompl_raceEth_count.rename(columns = {'DemC9': 'low compl.'}, inplace = True)\n",
    "\n",
    "raceEth_count = core_raceEth_count.merge(excl_raceEth_count, left_index = True, right_index=True, how = 'outer')\n",
    "raceEth_count  = raceEth_count.merge(lowcompl_raceEth_count, left_index = True, right_index=True, how = 'outer')\n",
    "\n",
    "                                 \n",
    "raceEth_prop = core_raceEth.merge(excl_raceEth, left_index = True, right_index=True, how = 'outer')\n",
    "raceEth_prop  = raceEth_prop.merge(lowcompl_raceEth, left_index = True, right_index=True, how = 'outer')\n",
    "raceEth_prop['abs diff. core - excl.'] = abs(raceEth_prop['core'] - raceEth_prop['excl.'])\n",
    "raceEth_prop['abs diff. core - low compl.'] = abs(raceEth_prop['core'] - raceEth_prop['low compl.'])\n",
    "\n",
    "\n",
    "raceEth_count.to_csv('attrition_raceEth_bracket_count.csv')\n",
    "\n",
    "for i in raceEth_count.index:\n",
    "    count = np.array([raceEth_count.loc[i, 'core'], raceEth_count.loc[i, 'excl.']])\n",
    "    nobs = np.array([raceEth_count.core.sum(), raceEth_count['excl.'].sum()])\n",
    "    stat, pval = proportions_ztest(count, nobs)\n",
    "    raceEth_prop.loc[i, 'z-val (core/excl.)'] = stat\n",
    "    raceEth_prop.loc[i, 'p-val (core/excl.)'] = pval\n",
    "    \n",
    "    count = np.array([raceEth_count.loc[i, 'core'], raceEth_count.loc[i, 'low compl.']])\n",
    "    nobs = np.array([raceEth_count.core.sum(), raceEth_count['low compl.'].sum()])\n",
    "    stat, pval = proportions_ztest(count, nobs)\n",
    "    raceEth_prop.loc[i, 'z-val (core/low compl.)'] = stat\n",
    "    raceEth_prop.loc[i, 'p-val (core/low compl.)'] = pval\n",
    "    \n",
    "raceEth_prop = raceEth_prop.astype(float).round(decimals=3)\n",
    "raceEth_prop.to_csv( 'attrition_raceEth_bracket_proportion.csv')\n",
    "\n",
    "raceEth_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "good_sub_data = data.set_index('PROLIFIC_PID')\n",
    "good_ids = core_sample_PID \n",
    "good_sub_data = good_sub_data.loc[list(good_ids)]\n",
    "good_sub_data.reset_index( inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_ids = core_sample_PID \n",
    "low_compl_ids = low_compl_sample_PID \n",
    "excl_ids = excl_sample_PID \n",
    "\n",
    "good_sub_data = data.set_index('PROLIFIC_PID')\n",
    "good_ids = core_sample_PID \n",
    "good_sub_data = good_sub_data.loc[list(good_ids)]\n",
    "good_sub_data.reset_index( inplace = True)\n",
    "\n",
    "\n",
    "###### age distribution ######\n",
    "column_extention = 'good'\n",
    "age_sex_good,_ = get_age_sex_data(good_ids, data, column_extention)\n",
    "column_extention = 'excl'\n",
    "age_sex_excl,_ = get_age_sex_data(excl_ids, data, column_extention)\n",
    "column_extention = 'low_compl'\n",
    "age_sex_low_comp,_ = get_age_sex_data(low_compl_ids, data, column_extention)\n",
    "age_sex = age_sex_good.join(age_sex_excl, how='outer')\n",
    "age_sex = age_sex.join(age_sex_low_comp, how='outer')\n",
    "age_sex = age_sex.fillna(0)\n",
    "\n",
    "# randomly draw nSamples to create a distribution of eucledian distances between a sampe of the same size as \n",
    "# exlcuded sample or the sample with a low completion rate and the sample of good subjects.\n",
    "age_rand_dist_male_excl = rand_euclDistance_dist_age(excl_ids, age_sex, good_sub_data, 'Male',nSamples, '_excl')\n",
    "age_rand_dist_male_low_comp = rand_euclDistance_dist_age(low_compl_ids, age_sex, good_sub_data, 'Male',nSamples,'_low_compl')\n",
    "age_rand_dist_female_excl = rand_euclDistance_dist_age(excl_ids, age_sex, good_sub_data, 'Female',nSamples, '_excl')\n",
    "age_rand_dist_female_low_comp = rand_euclDistance_dist_age(low_compl_ids, age_sex, good_sub_data, 'Female', nSamples, '_low_compl')\n",
    "\n",
    "\n",
    "\n",
    "###### political identity distribution ######\n",
    "party_exclude, _ = poli_affil(excl_ids, data,'_excl')\n",
    "party_low_comp_rate, _ = poli_affil(low_compl_ids, data,'_low_compl')\n",
    "party_good, _ = poli_affil(good_ids, data,'_good')\n",
    "party = party_good.join(party_exclude, how='outer')\n",
    "party = party.join(party_low_comp_rate, how='outer')\n",
    "party = party.fillna(0)\n",
    "\n",
    "# randomly draw nSamples to create a distribution of eucledian distances between a sampe of the same size as \n",
    "# exlcuded sample or the sample with a low completion rate and the sample of good subjects.\n",
    "party_rand_dist_excl = rand_euclDistance_dist_party(excl_ids, party_good, good_sub_data,  nSamples, 'excl')\n",
    "party_rand_dist_low_comp = rand_euclDistance_dist_party(low_compl_ids, party_good, good_sub_data,  nSamples, '_low_compl')\n",
    "\n",
    "\n",
    "\n",
    "######  income bracket distribution ###### \n",
    "income_exclude= income_brac(excl_ids, data,'_excl')\n",
    "income_low_comp_rate= income_brac(low_compl_ids, data,'_low_compl')\n",
    "income_good= income_brac(good_ids, data,'_good')\n",
    "income = income_good.join(income_exclude, how='outer')\n",
    "income = income.join(income_low_comp_rate, how='outer')\n",
    "income = income.fillna(0)\n",
    "\n",
    "# randcomly draw nSamples to create a distribution of eucledian distances between a sampe of the same size as \n",
    "# exlcuded sample or the sample with a low completion rate and the full week1 sample. \n",
    "income_rand_dist_excl = rand_euclDistance_dist_income(excl_ids, income_good, good_sub_data, nSamples, '_excl')\n",
    "income_rand_dist_low_comp = rand_euclDistance_dist_income(low_compl_ids, income_good, good_sub_data,  nSamples, '_low_compl')\n",
    "\n",
    "\n",
    "\n",
    "###### highest education level\n",
    "education_exclude= highest_education(excl_ids, data,'_excl')\n",
    "education_low_comp_rate= highest_education(low_compl_ids, data,'_low_compl')\n",
    "education_good= highest_education(good_ids, data,'_good')\n",
    "education = education_good.join(education_exclude, how='outer')\n",
    "education = education.join(education_low_comp_rate, how='outer')\n",
    "education = education.fillna(0)\n",
    "\n",
    "# randomly draw nSamples to create a distribution of eucledian distances between a sampe of the same size as \n",
    "# exlcuded sample or the sample with a low completion rate and the full week1 sample. \n",
    "education_rand_dist_excl = rand_euclDistance_dist_education(excl_ids, education_good, good_sub_data, nSamples,'_excl')\n",
    "education_rand_dist_low_comp = rand_euclDistance_dist_education(low_compl_ids, education_good, good_sub_data,  nSamples,'_low_compl')\n",
    "\n",
    "\n",
    "\n",
    "###### race and ethnicity \n",
    "raceEth_exclude= raceEthnicity(excl_ids, data,'_excl')\n",
    "raceEth_low_comp_rate= raceEthnicity(low_compl_ids, data,'_low_compl')\n",
    "raceEth_good= raceEthnicity(good_ids, data,'_good')\n",
    "raceEth = raceEth_good.join(raceEth_exclude, how='outer')\n",
    "raceEth = raceEth.join(raceEth_low_comp_rate, how='outer')\n",
    "raceEth = raceEth.fillna(0)\n",
    "\n",
    "# randomly draw nSamples to create a distribution of eucledian distances between a sampe of the same size as \n",
    "# exlcuded sample or the sample with a low completion rate and the full week1 sample. \n",
    "raceEth_rand_dist_excl = rand_euclDistance_dist_raceEthnicity(excl_ids, raceEth_good, good_sub_data, nSamples,'_excl')\n",
    "raceEth_rand_dist_low_comp = rand_euclDistance_dist_raceEthnicity(low_compl_ids, raceEth_good, good_sub_data,  nSamples,'_low_compl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save bootstrapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### age distribution ######\n",
    "# save to csv\n",
    "eucl_dist = pd.DataFrame()\n",
    "eucl_dist = age_rand_dist_female_low_comp.join(age_rand_dist_female_excl, how = 'outer')\n",
    "eucl_dist.loc[0,'true_eucl_dist_low_compl'] = np.linalg.norm(age_sex['Female_good']-age_sex['Female_low_compl'])\n",
    "eucl_dist.loc[0,'true_eucl_dist_excl'] = np.linalg.norm(age_sex['Female_good']-age_sex['Female_excl'])\n",
    "eucl_dist.to_csv(fdir_validation + 'attrition_age_female_distance_measures.csv')\n",
    "\n",
    "eucl_dist = pd.DataFrame()\n",
    "eucl_dist = age_rand_dist_male_low_comp.join(age_rand_dist_male_excl, how = 'outer')\n",
    "eucl_dist.loc[0,'true_eucl_dist_low_compl'] = np.linalg.norm(age_sex['Male_good']-age_sex['Male_low_compl'])\n",
    "eucl_dist.loc[0,'true_eucl_dist_excl'] = np.linalg.norm(age_sex['Male_good']-age_sex['Male_excl'])\n",
    "eucl_dist.to_csv(fdir_validation + 'attrition_age_male_distance_measures.csv')\n",
    "\n",
    "age_sex.to_csv(fdir_validation + 'attrition_age_sex.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### political identity distribution ######\n",
    "# save to csv\n",
    "eucl_dist = pd.DataFrame()\n",
    "eucl_dist = party_rand_dist_low_comp.join(party_rand_dist_excl, how = 'outer')\n",
    "eucl_dist.loc[0,'true_eucl_dist_low_compl'] = np.linalg.norm(party['party_good']-party['party_low_compl'])\n",
    "eucl_dist.loc[0,'true_eucl_dist_excl'] = np.linalg.norm(party['party_good']-party['party_excl'])\n",
    "\n",
    "eucl_dist.to_csv(fdir_validation + 'attrition_party_distance_measures.csv')\n",
    "party.to_csv(fdir_validation + 'attrition_party.csv')\n",
    "\n",
    "\n",
    "######  income bracket distribution ###### \n",
    "# save to csv\n",
    "eucl_dist = pd.DataFrame()\n",
    "eucl_dist = raceEth_rand_dist_low_comp.join(raceEth_rand_dist_excl, how = 'outer')\n",
    "eucl_dist.loc[0,'true_eucl_dist_low_compl'] = np.linalg.norm(income['raceEth_good']-income['raceEth_low_compl'])\n",
    "eucl_dist.loc[0,'true_eucl_dist_excl'] = np.linalg.norm(income['raceEth_good']-income['raceEth_excl'])\n",
    "\n",
    "eucl_dist.to_csv(fdir_validation + 'attrition_raceEth_distance_measures.csv')\n",
    "income.to_csv(fdir_validation + 'attrition_income.csv')\n",
    "\n",
    "\n",
    "\n",
    "###### highest education level###### \n",
    "# save to csv\n",
    "eucl_dist = pd.DataFrame()\n",
    "eucl_dist = education_rand_dist_low_comp.join(education_rand_dist_excl, how = 'outer')\n",
    "eucl_dist.loc[0,'true_eucl_dist_low_compl'] = np.linalg.norm(education['education_good']-education['education_low_compl'])\n",
    "eucl_dist.loc[0,'true_eucl_dist_excl'] = np.linalg.norm(education['education_good']-education['education_excl'])\n",
    "\n",
    "eucl_dist.to_csv(fdir_validation + 'attrition_education_distance_measures.csv')\n",
    "education.to_csv(fdir_validation + 'attrition_education.csv')\n",
    "\n",
    "\n",
    "###### race and ethnicity ###### \n",
    "# save to csv\n",
    "eucl_dist = pd.DataFrame()\n",
    "eucl_dist = raceEth_rand_dist_low_comp.join(raceEth_rand_dist_excl, how = 'outer')\n",
    "eucl_dist.loc[0,'true_eucl_dist_low_compl'] = np.linalg.norm(raceEth['raceEth_good']-raceEth['raceEth_low_compl'])\n",
    "eucl_dist.loc[0,'true_eucl_dist_excl'] = np.linalg.norm(raceEth['raceEth_good']-raceEth['raceEth_excl'])\n",
    "\n",
    "eucl_dist.to_csv(fdir_validation + 'attrition_raceEth_distance_measures.csv')\n",
    "raceEth.to_csv(fdir_validation + 'attrition_raceEth.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rc('axes', titlesize=30)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=30)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=30)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=30)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=30)    # legend fontsize\n",
    "plt.rc('figure', titlesize=50)  # fontsize of the figure title\n",
    "plt.rcParams['font.size'] = 30\n",
    "linewidth = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(40,60))\n",
    "\n",
    "\n",
    "gs = fig.add_gridspec(6,4)\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "ax2 = fig.add_subplot(gs[1,0])\n",
    "ax3 = fig.add_subplot(gs[0:2, 1])\n",
    "ax4 = fig.add_subplot(gs[0,2])\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "ax6 = fig.add_subplot(gs[0:2, 3])\n",
    "\n",
    "\n",
    "ax7 = fig.add_subplot(gs[2,0])\n",
    "ax8 = fig.add_subplot(gs[2,1])\n",
    "ax9 = fig.add_subplot(gs[2,2])\n",
    "ax10 = fig.add_subplot(gs[2,3])\n",
    "ax11 = fig.add_subplot(gs[3,0])\n",
    "ax12 = fig.add_subplot(gs[3,1])\n",
    "ax13 = fig.add_subplot(gs[3,2])\n",
    "ax14 = fig.add_subplot(gs[3,3])\n",
    "ax15 = fig.add_subplot(gs[4,0])\n",
    "ax16 = fig.add_subplot(gs[4,1])\n",
    "ax17 = fig.add_subplot(gs[4,2])\n",
    "ax18 = fig.add_subplot(gs[4,3])\n",
    "ax19 = fig.add_subplot(gs[5,0])\n",
    "ax20 = fig.add_subplot(gs[5,1])\n",
    "ax21 = fig.add_subplot(gs[5,2])\n",
    "ax22 = fig.add_subplot(gs[5,3])\n",
    "\n",
    "##### AGE SEX ########\n",
    "# reshape age sex data\n",
    "age_sex_plot = age_sex.copy()\n",
    "age_sex_plot['Female_good'] = -age_sex_plot['Female_good']\n",
    "age_sex_plot['Female_excl']= -age_sex_plot['Female_excl']\n",
    "age_sex_plot['Female_low_compl']= -age_sex_plot['Female_low_compl']\n",
    "\n",
    "# plot random distribution of age distances and true distance\n",
    "# male exluded\n",
    "hist_sampledEuclidDist(age_rand_dist_male_excl, np.linalg.norm(age_sex['Male_good']-age_sex['Male_excl']), ax1, '',linewidth)\n",
    "# female excluded\n",
    "hist_sampledEuclidDist(age_rand_dist_female_excl, np.linalg.norm(age_sex['Female_good']-age_sex['Female_excl']), ax2, '',linewidth)\n",
    "# male low rompletion rate\n",
    "hist_sampledEuclidDist(age_rand_dist_male_low_comp, np.linalg.norm(age_sex['Male_good']-age_sex['Male_low_compl']), ax4, '',linewidth)\n",
    "# female low completion rate\n",
    "hist_sampledEuclidDist(age_rand_dist_female_low_comp, np.linalg.norm(age_sex['Female_good']-age_sex['Female_low_compl']), ax5, '', linewidth)\n",
    "\n",
    "age_sex_butterfly_plot(age_sex_plot, 'excl', ax3)\n",
    "\n",
    "age_sex_butterfly_plot(age_sex_plot, 'low_compl', ax6)\n",
    "\n",
    "\n",
    "# party exluded\n",
    "party_colors=['royalblue','mediumpurple','indianred','yellow']\n",
    "hist_sampledEuclidDist(party_rand_dist_excl, np.linalg.norm(party['party_good']-party['party_excl']), ax7, 'party excl.', linewidth)\n",
    "# party low completion rate\n",
    "hist_sampledEuclidDist(party_rand_dist_low_comp, np.linalg.norm(party['party_good']-party['party_low_compl']), ax9, '', linewidth)\n",
    "# party exclude pie\n",
    "nested_pie(party, 'party_excl', 'party_good', ax8, 'party ID', party_colors)\n",
    "# party low completion rate pie\n",
    "nested_pie(party, 'party_low_compl', 'party_good', ax10, 'party ID', party_colors)\n",
    "\n",
    "# eduaction exluded\n",
    "cmap = plt.get_cmap('Set3')\n",
    "ed_colors = [cmap(i) for i in np.linspace(0, 1, len(education))]\n",
    "hist_sampledEuclidDist(education_rand_dist_excl, np.linalg.norm(education['education_good']-education['education_excl']), ax11, '',linewidth)\n",
    "# eduaction low completion rate\n",
    "hist_sampledEuclidDist(education_rand_dist_low_comp, np.linalg.norm(education['education_good']-education['education_low_compl']), ax13, '',linewidth)\n",
    "# eduaction exclude pie\n",
    "nested_pie(education, 'education_excl', 'education_good', ax12, 'highest education',ed_colors)\n",
    "# eduaction low completion rate pie\n",
    "nested_pie(education, 'education_low_compl', 'education_good', ax14, 'highest education',ed_colors)\n",
    "\n",
    "\n",
    "# income exluded\n",
    "cmap = plt.get_cmap('Set2')\n",
    "raceEth_colors = [cmap(i) for i in np.linspace(0, 1, len(income))]\n",
    "hist_sampledEuclidDist(raceEth_rand_dist_excl, np.linalg.norm(income['raceEth_good']-income['raceEth_excl']), ax15, '',linewidth)\n",
    "# income low completion rate\n",
    "hist_sampledEuclidDist(raceEth_rand_dist_low_comp, np.linalg.norm(income['raceEth_good']-income['raceEth_low_compl']), ax17, '',linewidth)\n",
    "# income exclude pie\n",
    "nested_pie(income, 'raceEth_excl', 'raceEth_good', ax16, '',raceEth_colors)\n",
    "# income low completion rate pie\n",
    "nested_pie(income, 'raceEth_low_compl', 'raceEth_good', ax18, '',raceEth_colors)\n",
    "\n",
    "# race/ethnicity exluded\n",
    "race_colors = ['MediumVioletRed','DarkViolet', 'SteelBlue','Gold','FireBrick','MediumPurple','Coral','MediumSeaGreen','DarkCyan']\n",
    "hist_sampledEuclidDist(raceEth_rand_dist_excl, np.linalg.norm(raceEth['raceEth_good']-raceEth['raceEth_excl']), ax19, '',linewidth)\n",
    "# race/ethnicity low completion rate\n",
    "hist_sampledEuclidDist(raceEth_rand_dist_low_comp, np.linalg.norm(raceEth['raceEth_good']-raceEth['raceEth_low_compl']), ax21, '',linewidth)\n",
    "# race/ethnicity exclude pie\n",
    "nested_pie(raceEth, 'raceEth_excl', 'raceEth_good', ax20, '',race_colors)\n",
    "# race/ethnicity low completion rate pie\n",
    "nested_pie(raceEth, 'raceEth_low_compl', 'raceEth_good', ax22, '',race_colors)\n",
    "\n",
    "\n",
    "fig.savefig('/Users/trusch/Box/COVID-19 Adolphs Lab/Visualization/attrition/attrition_all.svg',dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### age distribution ######\n",
    "column_extention = 'good'\n",
    "age_sex_good,_ = get_age_sex_data(good_ids, data, column_extention)\n",
    "column_extention = 'excl'\n",
    "age_sex_excl,_ = get_age_sex_data(excl_ids, data, column_extention)\n",
    "column_extention = 'low_compl'\n",
    "age_sex_low_comp,_ = get_age_sex_data(low_compl_ids, data, column_extention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_extention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi square test\n",
    "age_sex_all = pd.concat([age_sex_good,age_sex_excl, age_sex_low_comp],axis=1).fillna(0)\n",
    "\n",
    "\n",
    "# chi square test\n",
    "f_exp = age_sex_all['Male_good']\n",
    "f_obs = age_sex_all['Male_excl']\n",
    "chisq, p_val = stats.chisquare(f_obs,f_exp )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p, dof, ex = stats.chi2_contingency(age_sex_all.loc[:,['Male_good','Male_excl']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_sex_all.loc[:,['Male_good','Male_excl']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [str(i*5) + '-' + str((i+1)*5-1) for i in range(20)]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
