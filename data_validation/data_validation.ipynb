{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_w = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10','11', '12', '13','14', '15' ,'17', '18',\n",
    "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J','K', 'L', 'M', 'N']\n",
    "base_path =   os.path.expanduser('~/Box/COVID-19 Adolphs Lab/')\n",
    "data_dir = base_path+ '/PreProcessed_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_dir + 'Wave1-17_A-N_release.csv',encoding = 'ISO-8859-1',dtype=str, keep_default_na=False, na_values=['','NA'], low_memory=False)\n",
    "#data.PROLIFIC_PID = data.PROLIFIC_PID.str.strip('- ')\n",
    "print(sum(data.PROLIFIC_PID.str.endswith('-')))\n",
    "data = data.reindex(natsorted(data.columns), axis=1)\n",
    "data = data.loc[~data['PROLIFIC_PID'].isna()]\n",
    "\n",
    "attent_vars = ['NIHE1_9', 'DemC25', 'DISG1.2_23', 'DISG1.1_23', 'DISG2.2_23', 'ReSe1_23', 'Fed_13', 'RW23', 'GFPS2_11', 'EES1_32']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to create the validation file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = data.loc[:,['PROLIFIC_PID', 'wave']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = data.loc[:,'V4'].apply(pd.Timestamp)-data.loc[:,'V3'].apply(pd.Timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.loc[:,'total_time'] = s.apply(lambda x: x.seconds/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attention check questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task\n",
    "validation['task_completed'] = data['Tsk4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. response string length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def long_string(row):\n",
    "    # Return a list of response string length\n",
    "    # The calculation of MaxLongString, meanLongString and countLongString can be calculated from this list\n",
    "    count_ls = []\n",
    "    string_len = 1\n",
    "    for i in range(1, len(row)):\n",
    "        #print(i)\n",
    "        if np.isnan(float(row[i])):\n",
    "            # If the row contains all nas, then will append an empty list\n",
    "#             if string_len > 1:\n",
    "#                 count_ls.append(string_len)\n",
    "#                 string_len = 1\n",
    "            continue\n",
    "        elif row[i] == row[i-1]:\n",
    "            string_len += 1\n",
    "        elif row[i-1] is not np.nan:\n",
    "            count_ls.append(string_len)\n",
    "            string_len = 1\n",
    "    \n",
    "    if ~np.all(np.isnan([float(x) for x in row])):\n",
    "        count_ls.append(string_len)\n",
    "    return np.array(count_ls)\n",
    "\n",
    "\n",
    "def get_max(ls):\n",
    "    if ls.size == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return ls[np.argmax(ls)]\n",
    "    \n",
    "    \n",
    "def get_mean(ls):\n",
    "    if ls.size == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return np.mean(ls)\n",
    "    \n",
    "def count_greater_than_5(ls):\n",
    "    if ls.size == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return np.sum(ls >= 5)\n",
    "    \n",
    "def response_string(dat, name, validation):\n",
    "    s_long_string = dat.apply(long_string, axis=1)\n",
    "    validation['maxLongString_'+name] = s_long_string.apply(get_max)\n",
    "    validation['meanLongString_'+name] = s_long_string.apply(get_mean)\n",
    "    validation['countLongString_'+name] = s_long_string.apply(count_greater_than_5)\n",
    "    \n",
    "    return validation, s_long_string, ['maxLongString_'+name, 'meanLongString_'+name, 'countLongString_'+name]\n",
    "\n",
    "def inspect(dat, validation, s_long_string, input_vars, output_vars, inspect_w='1'):\n",
    "    print(dat.loc[dat['wave']==inspect_w, input_vars].head())\n",
    "    print(s_long_string[validation['wave']==inspect_w].head())\n",
    "    print(validation.loc[validation['wave']==inspect_w, output_vars].head())\n",
    "\n",
    "\n",
    "def process_na(dat, var_names):\n",
    "    for w in np.unique(dat['wave']):\n",
    "        for var in var_names:\n",
    "            if np.sum(~dat.loc[dat['wave']==w, var].isna())>0:\n",
    "                dat.loc[dat['wave']==w,var] = dat.loc[dat['wave']==w, var].fillna(0)\n",
    "        \n",
    "#         if((np.sum(np.sum(~dat.loc[dat['wave']==w, var_names].isna()))) > 0) and flag:\n",
    "#             dat.loc[dat['wave']==w, var_names] = dat.loc[dat['wave']==w, var_names].fillna(0)\n",
    "        \n",
    "    return dat\n",
    "\n",
    "def run_analysis(data, validation, name, input_vars, inspect_w=''):\n",
    "    data = process_na(data, input_vars)\n",
    "    input_dat = data.loc[:, input_vars]\n",
    "    validation, s, output_vars = response_string(input_dat, name, validation)\n",
    "    \n",
    "    if inspect_w:\n",
    "        inspect(data, validation, s, input_vars, output_vars, inspect_w=inspect_w)\n",
    "    \n",
    "    return validation, data\n",
    "\n",
    "\n",
    "def construct_vars(prefix, n, postfix=None, skip=[]):\n",
    "    ls = []\n",
    "    for i in range(n):\n",
    "        if (i+1) not in skip:\n",
    "            out = prefix + str(i+1)\n",
    "            if postfix:\n",
    "                out = out + postfix\n",
    "            ls.append(out)\n",
    "    return ls\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'EPII_infectHist_no'\n",
    "input_vars = \n",
    "validation, data = run_analysis(data, validation, name, input_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'EPII_infectHist_NA'\n",
    "input_vars = \n",
    "validation, data = run_analysis(data, validation, name, input_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'construct_vars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5dafb2dda7e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EPII_posChange_you'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EPII11_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'construct_vars' is not defined"
     ]
    }
   ],
   "source": [
    "name = 'EPII_posChange_you'\n",
    "input_vars = \n",
    "print(input_vars)\n",
    "validation, data = run_analysis(data, validation, name, input_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate interquartile range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interq_range(s, n):\n",
    "    # calculate the intequartile range given a pandas series - a list of responses\n",
    "    s_cleaned = s[~s.isna()]\n",
    "    if len(s_cleaned) == 0:\n",
    "        return [np.nan, np.nan]\n",
    "    else:\n",
    "        q1 = np.quantile(s_cleaned.astype(float), 0.25)\n",
    "        q3 = np.quantile(s_cleaned.astype(float), 0.75)\n",
    "        return [q1-n*(q3-q1), q3+n*(q3-q1)]\n",
    "\n",
    "def interq_analysis_old(dat, n_w, n_q, validation):\n",
    "    dat_tmp = dat.copy()\n",
    "    for w in n_w:\n",
    "        interq_r = dat.loc[validation['wave'] == w].apply(lambda x: interq_range(x, 1), axis=0)\n",
    "        print(interq_r)\n",
    "        for col in list(interq_r.index):\n",
    "            dat_no_na = dat.loc[(validation['wave'] == w) & (~dat[col].isna())]\n",
    "            if ~np.any(np.isnan(interq_r[col])):\n",
    "                idx = (validation['wave'] == w) & (dat_no_na[col]<=interq_r[col][1]) & (dat_no_na[col]>=interq_r[col][0])\n",
    "                dat_tmp.loc[idx, col] = 0\n",
    "\n",
    "    for q in range(n_q):\n",
    "        for w in n_w:\n",
    "            interq_r = dat.loc[validation['wave'] == w].apply(lambda x: interq_range(x, q+1), axis=0)\n",
    "            for col in list(interq_r.index):\n",
    "                dat_no_na = dat.loc[(validation['wave'] == w) & (~dat[col].isna())]\n",
    "                if ~np.any(np.isnan(interq_r[col])):\n",
    "                    idx = (validation['wave'] == w) & ((dat_no_na[col]<interq_r[col][0]) | (dat_no_na[col]>interq_r[col][1]))\n",
    "                    dat_tmp.loc[idx, col] = q+1\n",
    "    return dat_tmp\n",
    "\n",
    "\n",
    "def interq_analysis(dat, n_w, n_q, validation):\n",
    "    dat_tmp = dat.copy()\n",
    "    for w in n_w:\n",
    "        interq_r = dat.loc[validation['wave'] == w].apply(lambda x: interq_range(x, 1), axis=0)\n",
    "        for col in list(interq_r.columns):\n",
    "            dat_no_na = dat.loc[(validation['wave'] == w) & (~dat[col].isna())]\n",
    "            if ~np.any(np.isnan(interq_r[col])):\n",
    "                dat_no_na.head\n",
    "                idx = (validation['wave'] == w) & (dat_no_na[col]<=interq_r[col][1]) & (dat_no_na[col]>=interq_r[col][0])\n",
    "                dat_tmp.loc[idx, col] = 0\n",
    "\n",
    "    for q in range(n_q):\n",
    "        for w in n_w:\n",
    "            interq_r = dat.loc[validation['wave'] == w].apply(lambda x: interq_range(x, q+1), axis=0)\n",
    "            for col in list(interq_r.columns):\n",
    "                dat_no_na = dat.loc[(validation['wave'] == w) & (~dat[col].isna())]\n",
    "                if ~np.any(np.isnan(interq_r[col])):\n",
    "                    idx = (validation['wave'] == w) & ((dat_no_na[col]<interq_r[col][0]) | (dat_no_na[col]>interq_r[col][1]))\n",
    "                    dat_tmp.loc[idx, col] = q+1\n",
    "    return dat_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_mean = validation.filter(regex='meanLongString', axis=1)\n",
    "interq_rs_mean = interq_analysis(rs_mean, n_w, 3, validation)\n",
    "\n",
    "rs_max = validation.filter(regex='maxLongString', axis=1)\n",
    "interq_rs_max = interq_analysis(rs_max, n_w, 3, validation)\n",
    "\n",
    "interq_rs = pd.concat([interq_rs_mean, interq_rs_max], axis=1)\n",
    "interq_rs['wave'] = data['wave']\n",
    "interq_rs['PROLIFIC_PID'] = data['PROLIFIC_PID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Additional IQR variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data = pd.read_csv(base_path+'PreProcessed_Data/validation_files/additional_IQRmeasures_w1-17_wA-N.csv', index_col=0)\n",
    "p_dat = add_data.drop(['PROLIFIC_PID', 'wave'], axis=1)\n",
    "interq_add = interq_analysis(p_dat, n_w, 3, add_data)\n",
    "interq_add['wave'] = add_data['wave']\n",
    "interq_add['PROLIFIC_PID'] = add_data['PROLIFIC_PID']\n",
    "all_interq = interq_rs.merge(interq_add, on=['PROLIFIC_PID', 'wave'], how='left')\n",
    "interq_add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dat = pd.read_csv(base_path+'/PreProcessed_Data/validation_files/all_tasks_w1-17_A-N.csv',dtype=str, keep_default_na=False, na_values=['','nan'])\n",
    "miss_cols = list(task_dat.columns[task_dat.columns.str.endswith('_missing')])\n",
    "admin_cols = list(task_dat.columns[task_dat.columns.str.endswith( '_administered')])\n",
    "\n",
    "p_task_dat = task_dat[['tr_1s_rt_pctlt_300','tr_1s_medianRT','tr_1s_meanLongString','amp_pct_bad_rts', 'amp_medianRT','amp_meanLongString', \n",
    "        'altt_meanLongString', 'altt_medianRT', 'cvd_consp_meanLongString']]\n",
    "p_task_dat = p_task_dat.astype(float)\n",
    "interq_task = interq_analysis(p_task_dat, n_w, 3, task_dat)\n",
    "interq_task['wave'] = task_dat['wave']\n",
    "interq_task['PROLIFIC_PID'] = task_dat['PROLIFIC_PID']\n",
    "all_interq = all_interq.merge(interq_task, on=['PROLIFIC_PID', 'wave'], how='left')\n",
    "all_interq\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_interq = interq_rs.merge(interq_add, on=['PROLIFIC_PID', 'wave'], how='left')\n",
    "all_interq.to_csv(base_path+'/PreProcessed_Data/validation_files/all_IQRmeasures_output_w1-17_A-N.csv', index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:covid_base] *",
   "language": "python",
   "name": "conda-env-covid_base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
