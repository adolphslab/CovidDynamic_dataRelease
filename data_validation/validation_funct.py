{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeac189-8a71-4330-9ee7-c74d74e72aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cal_completed(incld, dat, w):\n",
    "#     finished = dat.loc[dat['wave']==str(w), ['PROLIFIC_PID', 'V5']]\n",
    "#     n_completed = len(finished.loc[finished['V5']=='1'])\n",
    "#     r_completed = n_completed/len(incld)\n",
    "#     return(n_completed, r_completed)\n",
    "\n",
    "# def incld_lst(s_in, s_w):\n",
    "#     #  Retrun a True False series indicating which record in w5 included list is not in the \n",
    "#     #  inputted included list (w1, w2, w3, w4)\n",
    "#     return s_in.isin(s_w)\n",
    "\n",
    "\n",
    "# def ext_nouns(txt):\n",
    "#     # A package call textblob allows us to easily extract nouns\n",
    "# #     if txt is np.nan:\n",
    "# #         return np.nan\n",
    "# #     else:\n",
    "# #         blob = TextBlob(txt)\n",
    "# #         return blob.noun_phrases\n",
    "#     # function to test if something is a noun\n",
    "#     if txt is np.nan:\n",
    "#         return np.nan\n",
    "#     else:\n",
    "#         is_noun = lambda pos: pos[:2] == 'NN'\n",
    "#         # do the nlp stuff\n",
    "#         tokenized = nltk.word_tokenize(txt)\n",
    "#         nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)]\n",
    "#         return nouns\n",
    "\n",
    "    \n",
    "# def ext_verbs(txt):\n",
    "\n",
    "#     if txt is np.nan:\n",
    "#         return np.nan\n",
    "#     else:\n",
    "#         is_noun = lambda pos: pos[:2] == 'VB'\n",
    "#         # do the nlp stuff\n",
    "#         tokenized = nltk.word_tokenize(txt)\n",
    "#         nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)]\n",
    "#         return nouns\n",
    "\n",
    "\n",
    "# def count_nouns(ls):\n",
    "#     if ls is np.nan:\n",
    "#         return np.nan\n",
    "#     else:\n",
    "#         return len(ls)\n",
    "\n",
    "# # Map the data collected in w1 to other weeks\n",
    "# def map_w1_to_other(col_name, validation):\n",
    "#     dictionary = validation.loc[validation['wave']=='1', ['PROLIFIC_PID', col_name]].set_index('PROLIFIC_PID').to_dict()\n",
    "#     validation.loc[validation[col_name].isna(), col_name] = validation.loc[validation[col_name].isna(), 'PROLIFIC_PID'].map(dictionary[col_name])\n",
    "#     return validation\n",
    "\n",
    "# # Map the prolific demographic data to the validation participants\n",
    "# def map_prolific_to_valid(prolific_name, prolific_demo, valid_name, validation):\n",
    "#     dictionary = prolific_demo.loc[:, ['participant_id', prolific_name]].set_index('participant_id').to_dict()\n",
    "#     validation[valid_name] = validation['PROLIFIC_PID'].map(dictionary[prolific_name])\n",
    "#     return validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54005830-5c16-4032-a06b-e56b1468721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attent_check(dat, w):\n",
    "    # Extract the responses for the attention check questions    \n",
    "    attent_vars = ['NIHE1_9', 'DemC25', \n",
    "                   'DISG1.2_23', 'DISG1.1_23','DISG2.2_23', \n",
    "                   'ReSe1_23', 'Fed_13', \n",
    "                   'RW23', 'GFPS2_11', 'EES1_32']\n",
    "    attent_df = dat.loc[dat['wave']==w, attent_vars]\n",
    "    attent_df = attent_df.dropna(axis=1, how='all')\n",
    "    # TODO: check whether the user fail the attention check questions.\n",
    "    # Attention check values\n",
    "    attent_chk = {'NIHE1_9': '4.0',\n",
    "              'DemC25': '3.0',\n",
    "              'DISG1.2_23': '4.0',\n",
    "              'DISG1.1_23': '2.0',\n",
    "              'DISG2.2_23': '4.0',\n",
    "              'ReSe1_23': '4.0',\n",
    "              'Fed_13': '7.0', \n",
    "              'RW23': '2.0',\n",
    "              'GFPS2_11': '1.0',\n",
    "              'EES1_32': '5.0',}\n",
    "    attent_cols = attent_df.columns\n",
    "    n = len(attent_df)\n",
    "    nan_array = np.empty(n)\n",
    "    nan_array[:] = np.nan\n",
    "    \n",
    "    out = pd.DataFrame({'failed_att_qns_1': nan_array,\n",
    "                        'failed_att_qns_2': nan_array,\n",
    "                        'failed_att_qns_3': nan_array,\n",
    "                        'failed_att_qns_4': nan_array,\n",
    "                        'failed_att_qns_5': nan_array})\n",
    "    out_cols = out.columns\n",
    "\n",
    "    \n",
    "    for i in range(len(attent_cols)):\n",
    "        tmp = (attent_df[attent_cols[i]] != attent_chk[attent_cols[i]])\n",
    "        out[out_cols[i]] = tmp.values\n",
    "    return out\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:covid_base] *",
   "language": "python",
   "name": "conda-env-covid_base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
