{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from valid_funct import *\n",
    "import importlib\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_export_path = '/Users/trusch/Box/COVID-19 Adolphs Lab/PreProcessed_Data/'\n",
    "data_export_filname = 'Wave1-18_A-N_release.csv'\n",
    "data = pd.read_csv(os.path.join(data_export_path, data_export_filname),dtype=str, keep_default_na=False, na_values=['','NA'], low_memory=False)\n",
    "task_data_path = '/Users/trusch/Box/COVID-19 Adolphs Lab/PreProcessed_Data/validation_files/all_tasks_w1-17_A-N.csv'\n",
    "\n",
    "listdir =  os.path.expanduser('~/Box/COVID-19 Adolphs Lab/participant_lists/')\n",
    "w2_list = pd.read_csv(listdir+'w2_includeList_postw1_analysis.csv', header=None)\n",
    "w3_list = pd.read_csv(listdir+'w3_includeList_postw2_analysis.csv', header=None)\n",
    "w4_list = pd.read_csv(listdir+'w4_includeList_postw3_analysis.csv', header=None)\n",
    "w5_list = pd.read_csv(listdir+'w5_includeList_postw4_analysis.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = data[['PROLIFIC_PID', 'wave']].copy() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Check Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attDf = pd.DataFrame()\n",
    "n_w = list(data['wave'].unique())\n",
    "for wave in n_w:\n",
    "    tmp = attent_check(data, wave)\n",
    "    attDf = pd.concat([attDf, tmp], ignore_index=True, sort=False)\n",
    "valid_data = valid_data.merge(attDf, on = ['PROLIFIC_PID', 'wave'], how = 'left')\n",
    "\n",
    "check_for_errors(data, valid_data, 'attention questions')\n",
    "del attDf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wave completed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_completed = w_completed(data)\n",
    "valid_data = valid_data.merge(w_completed, on = ['PROLIFIC_PID', 'wave'], how = 'left')\n",
    "\n",
    "check_for_errors(data, valid_data, 'wave completed')\n",
    "del w_completed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free response questions: at least one noun or one verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nounVerb_counts = nounVerb_count(data)\n",
    "valid_data = valid_data.merge(nounVerb_counts, on = ['PROLIFIC_PID', 'wave'], how = 'left')\n",
    "\n",
    "check_for_errors(data, valid_data, 'noun verb count')\n",
    "del nounVerb_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR of mean response string length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_string_df = data[['PROLIFIC_PID', 'wave']].copy()\n",
    "\n",
    "string_col_dict = {'PANA': data.loc[:,'PANA1_1':'PANA1_20'].columns,\n",
    "                'STAI_state': construct_vars('AnxS1_', 20),\n",
    "                'disgust_COVID': construct_vars('DISG1.2_', 22, skip=[13, 16]), \n",
    "                'fear_COVID': construct_vars('Fear2_', 7),\n",
    "                'EMSB': construct_vars('EMSB1_', 9),\n",
    "                'EMSC': construct_vars('EMSC1_', 9),\n",
    "                'NIHE':  construct_vars('NIHE1_', 8),\n",
    "                'NIHL': construct_vars('NIHL1_', 5),\n",
    "                'BDI': construct_vars('BDI', 21),\n",
    "                'CD_RISK': construct_vars('RISC1_', 10),\n",
    "                'consensBsl': construct_vars('cons1_', 35),\n",
    "                'consensCvd':construct_vars('cons2_', 20),\n",
    "                'EPII_infectHist_you':construct_vars('EPII10_', 8, '_1', skip=[7]),\n",
    "                'EPII_infectHist_inHome':construct_vars('EPII10_', 8, '_2', skip=[7]),\n",
    "                'EPII_infectHist_no':construct_vars('EPII10_', 8, '_3', skip=[7]),\n",
    "                'EPII_infectHist_NA': construct_vars('EPII10_', 8, '_4', skip=[7]),\n",
    "                'EPII_posChange_you': construct_vars('EPII11_', 18, '_1'),\n",
    "                'EPII_posChange_inHome': construct_vars('EPII11_', 18, '_2'),\n",
    "                'EPII_posChange_no': construct_vars('EPII11_', 18, '_3'), \n",
    "                'EPII_posChange_NA': construct_vars('EPII11_', 18, '_4'),\n",
    "                'EPII_work_you': construct_vars('EPII2_', 11, '_1'),\n",
    "                'EPII_work_inHome': construct_vars('EPII2_', 11, '_2'),\n",
    "                'EPII_work_no': construct_vars('EPII2_', 11, '_4'),\n",
    "                'EPII_work_NA': construct_vars('EPII2_', 11, '_5'),\n",
    "                'EPII_home_you': construct_vars('EPII4_', 13, '_1'),\n",
    "                'EPII_home_inHome': construct_vars('EPII4_', 13, '_2'),\n",
    "                'EPII_home_no': construct_vars('EPII4_', 13, '_3'),\n",
    "                'EPII_home_NA': construct_vars('EPII4_', 13, '_4'), \n",
    "                'EPII_socAct_you': construct_vars('EPII5_', 10, '_1'),\n",
    "                'EPII_socAct_inHome':construct_vars('EPII5_', 10, '_2'),\n",
    "                'EPII_socAct_no': construct_vars('EPII5_', 10, '_3'),\n",
    "                'EPII_socAct_NA': construct_vars('EPII5_', 10, '_4'),\n",
    "                'EPII_econ_you': construct_vars('EPII6_', 5, '_1'), \n",
    "                'EPII_econ_inHome': construct_vars('EPII6_', 5, '_2'), \n",
    "                'EPII_econ_no': construct_vars('EPII6_', 5, '_3'),\n",
    "                'EPII_econ_NA':  construct_vars('EPII6_', 5, '_4'),\n",
    "                'EPII_emo_you': construct_vars('EPII7_', 8, '_1', skip=[1,2]),\n",
    "                'EPII_emo_inHome': construct_vars('EPII7_', 8, '_2', skip=[1,2]),\n",
    "                'EPII_emo_no': construct_vars('EPII7_', 8, '_3'),\n",
    "                'EPII_emo_no': construct_vars('EPII7_', 8, '_4'),\n",
    "                'EPII_phys_you': construct_vars('EPII8_', 8, '_1'),\n",
    "                'EPII_phys_inHome': construct_vars('EPII8_', 8, '_2'),\n",
    "                'EPII_phys_no': construct_vars('EPII8_', 8, '_3'),\n",
    "                'EPII_phys_NA': construct_vars('EPII8_', 8, '_4'),\n",
    "                'EPII_dist_you': construct_vars('EPII9_', 8, '_1', skip=[8]),\n",
    "                'EPII_dist_inHome': construct_vars('EPII9_', 8, '_2', skip=[8]), \n",
    "                'EPII_dist_no': construct_vars('EPII9_', 8, '_3'), \n",
    "                'EPII_dist_NA': construct_vars('EPII9_', 8, '_4'),  \n",
    "                'trustPolit': construct_vars('RW6_1_', 11,skip=[7]),\n",
    "                'CovidImpact_inHouse': construct_vars('RW21_1_', 6),     \n",
    "                'CovidImpact_inHouse_v2': construct_vars('RW21v2_1_', 6),\n",
    "                'CovidImpact_hasCvd' : construct_vars('RW21_2_', 6) ,\n",
    "                'CovidImpact_hasCvd_v2': construct_vars('RW21v2_2_', 6),\n",
    "                'CovidImpact_posTest':  construct_vars('RW21_3_', 6), \n",
    "                'CovidImpact_posTest_v2': construct_vars('RW21v2_3_', 6),\n",
    "                'CovidImpact_hospital':  construct_vars('RW21_4_', 6), \n",
    "                'CovidImpact_hospital_v2':  construct_vars('RW21v2_4_', 6),\n",
    "                'CovidImpact_deceased':  construct_vars('RW21_5_', 6), \n",
    "                'CovidImpact_deceased_v2': construct_vars('RW21v2_5_', 6),\n",
    "                'CovidImpact_NA':  construct_vars('RW21_6_', 6), \n",
    "                'CovidImpact_NA_v2': construct_vars('RW21v2_6_', 6),\n",
    "                'approve': construct_vars('RW25_', 8), \n",
    "                'CvdPrevent': construct_vars('RW26_', 18),\n",
    "                'protest2': construct_vars('GFPS2_', 11, skip=[6]),\n",
    "                'protest2_v2': construct_vars('GFPS2v2_', 10, skip=[6]),\n",
    "                'protest4': construct_vars('GFPS4_', 7, skip=[3,5]),\n",
    "                'protest4_v2': construct_vars('GFPS4v2_', 7, skip=[3,5]),\n",
    "                'protest5': construct_vars('GFPS5_', 7, skip=[6]),\n",
    "                'protest5_v2': construct_vars('GFPS5v2_', 7, skip=[6]),\n",
    "                'protest6_happened': construct_vars('GFPS6_', 12, '_1', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_happened_v2': construct_vars('GFPS6v2_', 12, '_1', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_witnessed': construct_vars('GFPS6_', 12, '_3', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_witnessed_v2': construct_vars('GFPS6v2_', 12, '_3', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_learned': construct_vars('GFPS6_', 12, '_4', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_learned_v2': construct_vars('GFPS6v2_', 12, '_4', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_no': construct_vars('GFPS6_', 12, '_5', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_no_v2': construct_vars('GFPS6v2_', 12, '_5', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_noDisclose': construct_vars('GFPS6_', 12, '_6', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_noDisclose_v2': construct_vars('GFPS6v2_', 12, '_6', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_NA': construct_vars('GFPS6_', 12, '_7', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_NA_v2': construct_vars('GFPS6v2_', 12, '_7', skip=[1,2,3,5,6,8]),\n",
    "                'protest9': construct_vars('GFPS9_', 7),\n",
    "                'protest10': construct_vars('GFPS10_', 8),\n",
    "                'protest11':construct_vars('GFPS11_', 7),\n",
    "                'protest12':construct_vars('GFPS12_', 8),\n",
    "                'protest12_v2':construct_vars('GFPS12v2_', 8),\n",
    "                'protest13': construct_vars('GFPS13_', 7),\n",
    "                'protest14': construct_vars('GFPS14_', 8),\n",
    "                'protest15': construct_vars('GFPS15_', 7),\n",
    "                'protest16': construct_vars('GFPS16_', 8),\n",
    "                'protest17': construct_vars('GFPS17_', 12, skip=[4,5,7]),\n",
    "                'protest17_v2': construct_vars('GFPS17v2_', 12, skip=[4,5,7]),\n",
    "                'protest33': construct_vars('GFPS33_', 5),\n",
    "                'EES': construct_vars('EES1_', 31),\n",
    "                'Dscr': construct_vars('Dscr1_', 9),\n",
    "                'FWI': construct_vars('FWI1_', 15),\n",
    "                'hum': construct_vars('Hum1_', 10),\n",
    "                'PC': construct_vars('PC5.2_', 5),\n",
    "                'PPK': construct_vars('CvPP1_', 12),\n",
    "                'ResSe': construct_vars('ReSe1_', 22),\n",
    "                'SPS_city': construct_vars('City_', 12),\n",
    "                'SPS_state': construct_vars('State_', 12),\n",
    "                'SPS_fed': construct_vars('Fed_', 12),\n",
    "                'STAI_trait':  construct_vars('AnxT_', 20),\n",
    "                'VSA': construct_vars('VSA1_', 6),\n",
    "                'NEO1': construct_vars('NEO1_', 10),\n",
    "                'NEO2': construct_vars('NEO2_', 10),\n",
    "                'NEO3': construct_vars('NEO3_', 10),\n",
    "                'NEO4': construct_vars('NEO4_', 10),\n",
    "                'NEO5': construct_vars('NEO5_', 10),\n",
    "                'NEO6': construct_vars('NEO6_', 10),\n",
    "                'LEC_me': construct_vars('LEC1_', 17, '_1'),\n",
    "                'LEC_me_May': construct_vars('LEC1_May_', 17, '_1'),\n",
    "                'LEC_witness': construct_vars('LEC1_May_', 17, '_2'),\n",
    "                'LEC_witness_May': construct_vars('LEC1_', 17, '_2'),\n",
    "                'LEC_learned': construct_vars('LEC1_May_', 17, '_3'),\n",
    "                'LEC_learned_May': construct_vars('LEC1_', 17, '_3'),\n",
    "                'LEC_job': construct_vars('LEC1_', 17, '_4'),\n",
    "                'LEC_job_May': construct_vars('LEC1_May_', 17, '_4'),\n",
    "                'LEC_notSure': construct_vars('LEC1_', 17, '_5'),\n",
    "                'LEC_notSure_May': construct_vars('LEC1_May_', 17, '_5'),\n",
    "                'LEC_NA': construct_vars('LEC1_', 17, '_6'),\n",
    "                'LEC_NA_May': construct_vars('LEC1_May_', 17, '_6'),\n",
    "                'Cnsp': data.loc[:,'Cnsp1_1':'Cnsp4_8'].columns}\n",
    "for name in list(string_col_dict.keys()):\n",
    "    input_vars = string_col_dict[name]\n",
    "    long_string_df = long_string_df = extract_long_string(long_string_df, data, name, input_vars)\n",
    "\n",
    "iqr_rs_mean_cols = ['PROLIFIC_PID', 'wave'] +  list(long_string_df.columns[long_string_df.columns.str.startswith('meanLongString')])\n",
    "iqr_rs_mean = long_string_df[iqr_rs_mean_cols]\n",
    "iqr_rs_mean = interq_analysis(iqr_rs_mean, 3)\n",
    "iqr_rs_mean = iqr_rs_mean.replace({3.0: True, 2.0: False, 1.0: False, 0.0: False})\n",
    "\n",
    "# core questionnaires\n",
    "core_rs_questionnaires = ['meanLongString_PANA', 'meanLongString_STAI_state', 'meanLongString_STAI_trait', \n",
    "                          'meanLongString_NEO1', 'meanLongString_NEO2', 'meanLongString_NEO3', 'meanLongString_NEO4', \n",
    "                          'meanLongString_NEO5', 'meanLongString_NEO6', 'meanLongString_VSA', 'meanLongString_EES']\n",
    "\n",
    "# all questionnaires with >=4 questions\n",
    "all_rs_questionnaires = long_string_df.columns[long_string_df.columns.str.startswith('meanLongString')]\n",
    "iqr_rs_mean['string_outlier_core'] = iqr_rs_mean[core_rs_questionnaires].mean(axis=1, skipna = True)>=0.5\n",
    "iqr_rs_mean['string_outlier_all'] = iqr_rs_mean[all_rs_questionnaires].mean(axis=1, skipna = True)>=0.5\n",
    "\n",
    "valid_data = valid_data.merge(iqr_rs_mean[['PROLIFIC_PID','wave','string_outlier_core', 'string_outlier_all']], how = 'left', on = ['PROLIFIC_PID','wave'])\n",
    "\n",
    "check_for_errors(data, valid_data, 'questionnaire string outlier')\n",
    "del iqr_rs_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR of within questionnaire correlations: pos/neg items and regular and reverse scored items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_df = data[['PROLIFIC_PID', 'wave']].copy()\n",
    "\n",
    "PANAS_pos_idx = ['PANA1_1','PANA1_3','PANA1_5','PANA1_9','PANA1_10','PANA1_12','PANA1_14','PANA1_16',\n",
    "                 'PANA1_17','PANA1_19']\n",
    "PANAS_neg_idx = ['PANA1_2','PANA1_4','PANA1_6','PANA1_7','PANA1_8','PANA1_11','PANA1_13','PANA1_15',\n",
    "                 'PANA1_18','PANA1_20']\n",
    "\n",
    "PSS_idx = ['PSS1','PSS2','PSS3','PSS4','PSS5','PSS6','PSS7','PSS8','PSS9','PSS10']\n",
    "\n",
    "STAI_rev = ['AnxS1_1','AnxS1_2','AnxS1_5','AnxS1_8','AnxS1_10','AnxS1_11','AnxS1_15','AnxS1_16','AnxS1_19','AnxS1_20']\n",
    "STAI_reg = ['AnxS1_3', 'AnxS1_4', 'AnxS1_6', 'AnxS1_7', 'AnxS1_9', 'AnxS1_12', 'AnxS1_13','AnxS1_14', 'AnxS1_17','AnxS1_18']\n",
    "\n",
    "EES_empFeelExp_reg = ['EES1_3','EES1_9', 'EES1_11', 'EES1_12', 'EES1_13', 'EES1_14', 'EES1_15', 'EES1_18','EES1_22','EES1_23','EES1_26', 'EES1_30']\n",
    "EES_empFeelExp_rev = ['EES1_16','EES1_17', 'EES1_21']\n",
    "\n",
    "EES_empPers_reg = ['EES1_4','EES1_6', 'EES1_19']\n",
    "EES_empPers_rev = ['EES1_2','EES1_28', 'EES1_29', 'EES1_31']\n",
    "\n",
    "Disg_reg = ['DISG1.1_2', 'DISG1.1_3','DISG1.1_4', 'DISG1.1_5', 'DISG1.1_7', 'DISG1.1_8','DISG1.1_9', 'DISG1.1_12', 'DISG1.1_14', 'DISG1.1_16',\n",
    "            'DISG1.2_1', 'DISG1.2_3', 'DISG1.2_4', 'DISG1.2_5', 'DISG1.2_6', 'DISG1.2_7', 'DISG1.2_8', 'DISG1.2_9', 'DISG1.2_10', 'DISG1.2_11',\n",
    "            'DISG1.2_12', 'DISG1.2_14']\n",
    "Disg_rev = ['DISG1.1_1', 'DISG1.1_6','DISG1.1_10', 'DISG1.1_13', 'DISG1.2_2']\n",
    "\n",
    "coherence_df['PANAS_diff'] = abs(data[PANAS_neg_idx].astype(float).mean(axis=1) - data[PANAS_pos_idx].astype(float).mean(axis=1))\n",
    "coherence_df['PANASpos_PSS_diff'] = abs(data[PSS_idx].astype(float).mean(axis=1) - data[PANAS_pos_idx].astype(float).mean(axis=1))\n",
    "coherence_df['STAI_diff'] = abs(data[STAI_rev].astype(float).mean(axis=1) - data[STAI_reg].astype(float).mean(axis=1))\n",
    "coherence_df['EES_empFeelExp_diff'] = abs(data[EES_empFeelExp_rev].astype(float).mean(axis=1) - data[EES_empFeelExp_reg].astype(float).mean(axis=1))\n",
    "coherence_df['EES_empPers_diff'] = abs(data[EES_empPers_rev].astype(float).mean(axis=1) - data[EES_empPers_reg].astype(float).mean(axis=1))\n",
    "coherence_df['Disg_diff'] = abs(data[Disg_rev].astype(float).mean(axis=1) - data[Disg_reg].astype(float).mean(axis=1))\n",
    "\n",
    "iqr_coherence = interq_analysis(coherence_df, 3)\n",
    "iqr_coherence = iqr_coherence.replace({3.0: True, 2.0: False,1.0:  False, 0.0: False})\n",
    "iqr_coherence['response_consistency'] = iqr_coherence[['PANAS_diff','PANASpos_PSS_diff','STAI_diff',\n",
    "                                         'EES_empFeelExp_diff', 'EES_empPers_diff','Disg_diff']].mean(axis=1, skipna = True)>=0.5\n",
    "\n",
    "valid_data = valid_data.merge(iqr_coherence[['PROLIFIC_PID','wave','response_consistency']], how = 'left', on = ['PROLIFIC_PID','wave'])\n",
    "\n",
    "check_for_errors(data, valid_data, 'questionnaire coherence')\n",
    "del iqr_coherence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency of nan responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trust opinions -> not sure\n",
    "tmp = pd.DataFrame()\n",
    "nanResp_df = pd.DataFrame()\n",
    "nanResp_df = data[['PROLIFIC_PID', 'wave']].copy()\n",
    "\n",
    "tmp = data[['RW6_1_1','RW6_1_2', 'RW6_1_3', 'RW6_1_4', 'RW6_1_5', 'RW6_1_6', 'RW6_1_8', 'RW6_1_9', 'RW6_1_10', 'RW6_1_11']] == '7.0'\n",
    "tmp[data[['RW6_1_1','RW6_1_2', 'RW6_1_3', 'RW6_1_4', 'RW6_1_5', 'RW6_1_6', 'RW6_1_8', 'RW6_1_9', 'RW6_1_10', 'RW6_1_11']].isnull()] = np.nan\n",
    "nanResp_df['NaN_Freq1'] = tmp.mean(axis = 1)\n",
    "del tmp\n",
    "\n",
    "# important to prevent the spread of Covid -> not sure\n",
    "tmp = pd.DataFrame()\n",
    "tmp = data[['RW25_1', 'RW25_2', 'RW25_3', 'RW25_4', 'RW25_5', 'RW25_6', 'RW25_7', 'RW25_8']] == '7.0'\n",
    "tmp[data[['RW25_1', 'RW25_2', 'RW25_3', 'RW25_4', 'RW25_5', 'RW25_6', 'RW25_7', 'RW25_8']].isnull()] = np.nan\n",
    "nanResp_df['NaN_Freq2'] = tmp.mean(axis = 1)\n",
    "del tmp\n",
    "\n",
    "# Public Policy knowledge -> I don't know\n",
    "tmp = pd.DataFrame()\n",
    "tmp = data[['CvPP1_1','CvPP1_2','CvPP1_3','CvPP1_4','CvPP1_5','CvPP1_6','CvPP1_7','CvPP1_8','CvPP1_9','CvPP1_10','CvPP1_11','CvPP1_12']] == '4.0'\n",
    "tmp[data[['CvPP1_1','CvPP1_2','CvPP1_3','CvPP1_4','CvPP1_5','CvPP1_6','CvPP1_7','CvPP1_8','CvPP1_9','CvPP1_10','CvPP1_11','CvPP1_12']].isnull()] = np.nan\n",
    "nanResp_df['NaN_Freq3'] = tmp.mean(axis = 1)\n",
    "del tmp\n",
    "\n",
    "iqr_nanResp = interq_analysis(nanResp_df, 3)\n",
    "iqr_nanResp = iqr_nanResp.replace({3.0: True, 2.0: False,1.0:  False, 0.0: False})\n",
    "# NAN resp in half or more than half of the questions including NAN responses\n",
    "iqr_nanResp['freq_NAresp'] = iqr_nanResp[['NaN_Freq1','NaN_Freq2','NaN_Freq3']].mean(axis=1, skipna = True)>=0.5\n",
    "\n",
    "# add to validation data\n",
    "valid_data = valid_data.merge(iqr_nanResp[['PROLIFIC_PID','wave','freq_NAresp']], how = 'left', on = ['PROLIFIC_PID','wave'])\n",
    "\n",
    "check_for_errors(data, valid_data, 'NaN resps')\n",
    "del iqr_nanResp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "response string length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% run ProcessTaskData.ipynb\n",
    "task_data = pd.read_csv(task_data_path,dtype=str, low_memory=False)\n",
    "rs_cols = list(task_data.columns[task_data.columns.str.endswith('_meanLongString')])\n",
    "task_data[rs_cols] =task_data[rs_cols].astype(float)\n",
    "rs_cols = ['PROLIFIC_PID','wave'] + rs_cols \n",
    "\n",
    "iqr_task_rs = interq_analysis(task_data[rs_cols],3)\n",
    "iqr_task_rs = iqr_task_rs.replace({3.0: True, 2.0: False,1.0:  False, 0.0: False})\n",
    "iqr_task_rs  = iqr_task_rs.rename(columns={\"tr_1s_meanLongString\": \"string_outlier_tr\", \n",
    "                                            \"amp_meanLongString\": \"string_outlier_amp\",\n",
    "                                            \"altt_meanLongString\": \"string_outlier_altt\",\n",
    "                                            \"cvd_consp_meanLongString\": \"string_outlier_cvd_consp\"})\n",
    "\n",
    "valid_data = valid_data.merge(iqr_task_rs, how = 'left', on = ['PROLIFIC_PID','wave'])\n",
    "\n",
    "check_for_errors(data, valid_data, 'task rs')\n",
    "del iqr_task_rs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other task quality measures: RT outliers, iat/biat exclusion, tasks missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pctl_bad_rt_cutoff = 0.1\n",
    "\n",
    "task_valid = task_data[['PROLIFIC_PID','wave']].copy()\n",
    "\n",
    "# read in additional task quality measures: \n",
    "task_valid.loc[task_data['tr_1s_rt_pctlt_300'].astype(float)>=pctl_bad_rt_cutoff,'tr_1s_rt_pctlt_300'] = True\n",
    "task_valid.loc[task_data['tr_1s_rt_pctlt_300'].isnull(),'tr_1s_rt_pctlt_300'] = pd.NA;\n",
    "\n",
    "task_valid.loc[task_data['altt_rt_pctlt_300'].astype(float)>=pctl_bad_rt_cutoff,'altt_rt_pctlt_300'] = True\n",
    "task_valid.loc[task_data['altt_rt_pctlt_300'].isnull(),'altt_rt_pctlt_300'] = pd.NA;\n",
    "\n",
    "task_valid.loc[task_data['cvd_consp_rt_pctlt_300'].astype(float)>=pctl_bad_rt_cutoff,'cvd_consp_rt_pctlt_300'] = True\n",
    "task_valid.loc[task_data['cvd_consp_rt_pctlt_300'].isnull(),'cvd_consp_rt_pctlt_300'] = pd.NA;\n",
    "\n",
    "task_valid.loc[task_data['amp_pct_bad_rts'].astype(float)>=pctl_bad_rt_cutoff,'amp_pct_bad_rts'] = True\n",
    "task_valid.loc[task_data['amp_pct_bad_rts'].isnull(),'amp_pct_bad_rts'] = pd.NA;\n",
    "\n",
    "task_valid.loc[task_data['tr_1s_noVar']=='1.0','tr_1s_noVar'] = True\n",
    "task_valid.loc[task_data['tr_1s_noVar'].isnull(),'tr_1s_noVar'] = pd.NA;\n",
    "\n",
    "task_valid.loc[task_data['iat_include']=='0.0','iat_exclude'] = True\n",
    "task_valid.loc[task_data['iat_include'].isnull(),'iat_exclude'] = pd.NA;\n",
    "\n",
    "task_valid.loc[task_data['biat_include']=='0.0','biat_exclude'] = True\n",
    "task_valid.loc[task_data['biat_include'].isnull(),'biat_exclude'] = pd.NA;\n",
    "\n",
    "task_valid['biat_missing'] = task_data.biat_missing\n",
    "task_valid['altt_missing'] = task_data.altt_missing\n",
    "task_valid['tr_1s_missing'] = task_data.tr_1s_missing\n",
    "task_valid['iat_missing'] = task_data.iat_missing\n",
    "task_valid['pgg_missing'] = task_data.pgg_missing\n",
    "task_valid['amp_missing'] = task_data.amp_missing\n",
    "task_valid['cvd_consp_missing'] = task_data.cvd_consp_missing\n",
    "\n",
    "task_valid['biat_administered'] = task_data.biat_administered\n",
    "task_valid['altt_administered'] = task_data.altt_administered\n",
    "task_valid['tr_1s_administered'] = task_data.tr_1s_administered\n",
    "task_valid['iat_administered'] = task_data.iat_administered\n",
    "task_valid['pgg_administered'] = task_data.pgg_administered\n",
    "task_valid['amp_administered'] = task_data.amp_administered\n",
    "task_valid['cvd_consp_administered'] = task_data.cvd_consp_administered\n",
    "\n",
    "# rename rt coumns for consistency\n",
    "task_valid = task_valid.rename(columns={\"tr_1s_rt_pctlt_300\": \"tr_pct_bad_rts\",\n",
    "                                              \"altt_rt_pctlt_300\": \"altt_pct_bad_rts\",\n",
    "                                              \"cvd_consp_rt_pctlt_300\": \"cvd_consp_pct_bad_rts\"})\n",
    "\n",
    "# task missing/ administered \n",
    "tasks = ['altt','pgg','tr_1s','iat','amp','biat', 'cvd_consp']\n",
    "for w in task_valid.wave.unique():\n",
    "    for task in tasks:\n",
    "        if sum(task_valid.loc[task_valid.wave == w, task + '_administered'] == 'True')> 0:\n",
    "            task_valid.loc[task_valid.wave == w, task+ '_administered'] = 'True'\n",
    "        else:\n",
    "            task_valid.loc[task_valid.wave == w, task+ '_administered'] = 'False'\n",
    "        \n",
    "for task in tasks:\n",
    "    task_valid.loc[(task_valid[task+'_missing'].isnull()) &\n",
    "                                     (task_valid[task+'_administered'] == 'True'), task+'_missing'] = 'True'\n",
    "    task_valid.loc[(task_valid[task+'_missing'].isnull()) &\n",
    "                                     (task_valid[task+'_administered'] == 'False'), task+'_missing'] = 'False'\n",
    "# merge\n",
    "valid_data = valid_data.merge(task_valid, how = 'left', on = ['PROLIFIC_PID','wave'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# percentage of completed waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_prlfc = []\n",
    "for w in range(1,19,1):\n",
    "    w_prlfc.append(str(w))\n",
    "w_conte =['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J','K', 'L', 'M']  \n",
    "\n",
    "valid_data_prlfc = valid_data.loc[valid_data['wave'].isin(w_prlfc),:].copy()\n",
    "valid_data_conte = valid_data.loc[valid_data['wave'].isin(w_conte),:].copy()\n",
    "del valid_data\n",
    "\n",
    "wave_count_prlfc = valid_data_prlfc[['PROLIFIC_PID','wave']].groupby('PROLIFIC_PID').count()  \n",
    "wave_count_prlfc = wave_count_prlfc.rename(columns={'wave': 'nCompleted'}).reset_index()\n",
    "wave_count_prlfc['perc_completed'] = wave_count_prlfc.nCompleted/wave_count_prlfc.nCompleted.max()\n",
    "valid_data_prlfc = valid_data_prlfc.merge(wave_count_prlfc, on = 'PROLIFIC_PID', how = 'left')\n",
    "\n",
    "wave_count_conte = valid_data_conte[['PROLIFIC_PID','wave']].groupby('PROLIFIC_PID').count()    \n",
    "wave_count_conte = wave_count_conte.rename(columns={'wave': 'nCompleted'})\n",
    "wave_count_conte['perc_completed'] = wave_count_conte.nCompleted/wave_count_conte.nCompleted.max()\n",
    "valid_data_conte = valid_data_conte.merge(wave_count_conte, on = 'PROLIFIC_PID', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "week_sub_prlfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weekly subject count based on (cummulative) quality criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'week_sub_conte' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b4f7e3bf69cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mweek_sub_prlfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/Box/COVID-19 Adolphs Lab/PreProcessed_Data/validation_files/validation_proportion_prct_pass_perWave_w1-18_conte.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mweek_sub_conte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/Box/COVID-19 Adolphs Lab/PreProcessed_Data/validation_files/validation_proportion_prct_pass_perWave_wA-N_conte.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'week_sub_conte' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "validation_criteria =  ['more_than_1_attQ_failed', 'string_outlier_all', 'string_outlier_core',\n",
    "                            'string_outlier_amp', 'string_outlier_altt', 'string_outlier_cvd_consp', 'tr_pct_bad_rts','response_consistency',\n",
    "                            'altt_pct_bad_rts', 'amp_pct_bad_rts', 'cvd_consp_pct_bad_rts', 'iat_exclude', 'biat_exclude','free_text_resp_valid_stress',\n",
    "                            'free_text_resp_valid_news','freq_NAresp']\n",
    "    \n",
    "# drop all rows of incomplete qualtrics data \n",
    "valid_data_prlfc = valid_data_prlfc.loc[valid_data_prlfc['completed']=='1',:]\n",
    "valid_data_conte = valid_data_conte.loc[valid_data_conte['completed']=='1',:]\n",
    "    \n",
    "valid_data_prlfc['perc_valid_failed'] = valid_data_prlfc[validation_criteria].mean(axis = 1, skipna = True)\n",
    "valid_data_conte['perc_valid_failed'] = valid_data_prlfc[validation_criteria].mean(axis = 1, skipna = True)\n",
    "\n",
    "\n",
    "# wave-by-wave summary \n",
    "week_sub_prlfc = pd.DataFrame(index = list(range(1,19)))\n",
    "week_sub_prlfc['N'] = list(valid_data_prlfc.wave.value_counts())\n",
    "for wave in valid_data_prlfc.wave.unique():\n",
    "    for perc in list(np.arange(1, 0.45, -0.05)):\n",
    "        week_sub_prlfc.loc[int(wave),str(round(perc*100))+'%'] = round((sum(valid_data_prlfc.loc[valid_data_prlfc.wave == wave,'perc_valid_failed']<=(1-perc))/sum(valid_data_prlfc.wave == wave))*100,2)\n",
    "\n",
    "\n",
    "        \n",
    "valid_data_prlfc.to_csv(os.path.expanduser('~/Box/COVID-19 Adolphs Lab/PreProcessed_Data/validation_files/validation_passCriterion_perSub_perWave_w1-18_prlfc.csv'),index=False)\n",
    "valid_data_conte.to_csv(os.path.expanduser('~/Box/COVID-19 Adolphs Lab/PreProcessed_Data/validation_files/validation_passCriterion_perSub_perWave_wA-N_conte.csv'),index=False)\n",
    "\n",
    "week_sub_prlfc.to_csv(os.path.expanduser('~/Box/COVID-19 Adolphs Lab/PreProcessed_Data/validation_files/validation_proportion_prct_pass_perWave_w1-18_conte.csv'))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:covid_base] *",
   "language": "python",
   "name": "conda-env-covid_base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
